"","Title","Author","Subject","Abstract","Meta"
"1","Language Imbalance Can Boost Cross-lingual Generalisation","Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag","Computation and Language (cs.CL)","Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","Thu, 11 Apr 2024 17:58:05 UTC (4,890 KB)"
"2","OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","Artificial Intelligence (cs.AI)","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)"
"3","An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting","Chufeng Li, Jianyong Chen","Statistical Finance (q-fin.ST)","As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available at this https URL.","Mon, 25 Mar 2024 15:23:22 UTC (1,088 KB)"
"4","Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance","Xiangpeng Li, Ali Mostafavi","Computers and Society (cs.CY)","There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.","Sun, 24 Mar 2024 19:32:23 UTC (2,889 KB)"
"5","Visualization for physics analysis improvement and applications in BESIII","Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao","Data Analysis, Statistics and Probability (physics.data-an)","Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.","Tue, 19 Mar 2024 10:26:48 UTC (1,990 KB)"
"6","Distributed Record Linkage in Healthcare Data with Apache Spark","Mohammad Heydari, Reza Sarshar, Mohammad Ali Soltanshahi","Distributed, Parallel, and Cluster Computing (cs.DC)","Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.","Sat, 9 Mar 2024 05:18:15 UTC (249 KB)"
"7","Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","Charis Stamouli, Ingvar Ziemann, George J. Pappas","Statistics Theory (math.ST)","We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","Thu, 11 Apr 2024 17:36:28 UTC (45 KB)"
"8","Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","Tuhin Malik, V. Dexheimer, Constança Providência","Nuclear Theory (nucl-th)","We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\omega^2\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","Thu, 11 Apr 2024 17:35:54 UTC (1,088 KB)"
"9","Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","Kamal Aghazade, Ali Gholami, Hossein S. Aghamiry, Hamid Reza Siahkoohi","Numerical Analysis (math.NA)","Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","Thu, 11 Apr 2024 17:25:23 UTC (16,221 KB)"
"10","Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld","Human-Computer Interaction (cs.HC)","Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","Thu, 11 Apr 2024 16:40:24 UTC (1,038 KB)"
"11","Revisiting a drag partition model for canopy-like roughness elements","Elia Buono, Gabriel G. Katul, Davide Vettori, Davide Poggi, Costantino Manes","Fluid Dynamics (physics.flu-dyn)","Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","Thu, 11 Apr 2024 16:27:43 UTC (359 KB)"
"12","Diagram Analysis of Iterative Algorithms","Chris Jones, Lucas Pesenti","Computational Complexity (cs.CC)","We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.
We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.
The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by ""implementing"" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.
These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","Thu, 11 Apr 2024 16:10:52 UTC (1,097 KB)"
"13","The Power of Properties: Uncovering the Influential Factors in Emotion Classification","Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler","Computer Vision and Pattern Recognition (cs.CV)","Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","Thu, 11 Apr 2024 16:01:00 UTC (895 KB)"
"14","Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","Geng Chen, Faris A. El-Katri, Yanbo Hu, Yannan Shen","Analysis of PDEs (math.AP)","In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","Thu, 11 Apr 2024 15:15:35 UTC (34 KB)"
"15","iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang","Systems and Control (eess.SY)","This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","Thu, 11 Apr 2024 15:10:10 UTC (898 KB)"
"16","Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","Benjamin Lieberman, Andreas Crivellin, Salah-Eddine Dahbi, Finn Stevenson, Nidhi Tripathi, Mukesh Kumar, Bruce Mellado","High Energy Physics - Phenomenology (hep-ph)","To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","Thu, 11 Apr 2024 15:06:15 UTC (2,225 KB)"
"17","The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu","Analysis of PDEs (math.AP)","We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.
The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","Thu, 11 Apr 2024 14:52:25 UTC (70 KB)"
"18","Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann","Computer Vision and Pattern Recognition (cs.CV)","Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","Thu, 11 Apr 2024 14:00:20 UTC (23,169 KB)"
"19","3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","Xuanming Cao, Chengyu Tao, Juan Du","Computer Vision and Pattern Recognition (cs.CV)","The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","Thu, 11 Apr 2024 13:46:05 UTC (6,818 KB)"
"20","Depth Estimation using Weighted-loss and Transfer Learning","Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah","Computer Vision and Pattern Recognition (cs.CV)","Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","Thu, 11 Apr 2024 12:25:54 UTC (2,233 KB)"
"21","Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman","Computer Vision and Pattern Recognition (cs.CV)","Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","Thu, 11 Apr 2024 12:24:47 UTC (10,043 KB)"
"22","Merger Analysis with Latent Price","Paul Koh","Econometrics (econ.EM)","Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).","Thu, 11 Apr 2024 12:23:57 UTC (43 KB)"
"23","Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","Marc Hallin, Simos G. Meintanis, Klaus Nordhausen","Methodology (stat.ME)","We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","Thu, 11 Apr 2024 10:48:23 UTC (593 KB)"
"24","Lower semicontinuity and existence results for anisotropic TV functionals with signed measure data","Eleonora Ficola, Thomas Schmidt","Analysis of PDEs (math.AP)","We study the minimization of anisotropic total variation functionals with additional measure terms among functions of bounded variation subject to a Dirichlet boundary condition. More specifically, we identify and characterize certain isoperimetric conditions, which prove to be sharp assumptions on the signed measure data in connection with semicontinuity, existence, and relaxation results. Furthermore, we present a variety of examples which elucidate our assumptions and results.","Thu, 11 Apr 2024 10:44:07 UTC (270 KB)"
"25","State-Space Modeling of Shape-constrained Functional Time Series","Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa","Applications (stat.AP)","Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.","Thu, 11 Apr 2024 09:17:30 UTC (1,484 KB)"
"26","M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu","Information Retrieval (cs.IR)","We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","Thu, 11 Apr 2024 09:13:52 UTC (2,290 KB)"
"27","GAN-based iterative motion estimation in HASTE MRI","Mathias S. Feinler, Bernadette N. Hahn","Numerical Analysis (math.NA)","Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","Thu, 11 Apr 2024 09:07:57 UTC (6,372 KB)"
"28","Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","Alessandro Candido, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo","High Energy Physics - Phenomenology (hep-ph)","We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","Thu, 11 Apr 2024 09:03:52 UTC (442 KB)"
"29","Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","Claudia Toci, Simone Ceppi, Nicolás Cuello, Gaspard Duchêne, Enrico Ragusa, Giuseppe Lodato, Francesca Farina, François Ménard, Hossam Aly","Earth and Planetary Astrophysics (astro-ph.EP)","GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\mu$m ALMA dust continuum emission and 1.67 $\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\Delta\theta= 30^\circ$) on timescales shorter than $\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","Thu, 11 Apr 2024 08:51:50 UTC (9,675 KB)"
"30","X-ray polarimetric features of Gamma-ray Bursts across varied redshifts and hints for Axion-Like-Particles","Qingxiang Zhang, Feng Huang, Zhongxiang Wang, Taotao Fang","High Energy Astrophysical Phenomena (astro-ph.HE)","Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs. However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints. In this work, we first examine the statistical characteristics of linear polarization degree ($\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions. Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies. We then explore alternations to the initial $\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\bf B}_{\rm IGM} $). With the existence of ALPs with $m_{a}$$~$$\lesssim$$~$$10^{-14}$$~$eV and $g_{a\gamma}~$$\simeq$$~0.5\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons. To ensure that the effect of mixing is small enough to be negligible, the mixing term $\Delta_{a\gamma} \equiv 1/2\ g_{a\gamma} {\bf B}_{\rm IGM}$ should be less than $1.5\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited. Certification of redshift for GRBs with low $\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\bf B}_{\rm IGM}$.","Thu, 11 Apr 2024 08:36:21 UTC (2,354 KB)"
"31","How is Visual Attention Influenced by Text Guidance? Database and Model","Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai","Computer Vision and Pattern Recognition (cs.CV)","The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: this https URL.","Thu, 11 Apr 2024 08:03:23 UTC (3,122 KB)"
"32","On the convergence analysis of one-shot inversion methods","Marcella Bonazzoli (IDEFIX), Houssem Haddar (IDEFIX), Tuan Anh Vu (IDEFIX)","Numerical Analysis (math.NA)","When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively. The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods. We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions. Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem. We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter. We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations. Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead. We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data.","Thu, 11 Apr 2024 07:40:41 UTC (3,679 KB)"
"33","Characterizing the Influence of Topology on Graph Learning Tasks","Kailong Wu, Yule Xie, Jiaxin Ding, Yuxiang Ren, Luoyi Fu, Xinbing Wang, Chenghu Zhou","Machine Learning (cs.LG)","Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives. We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning.","Thu, 11 Apr 2024 06:04:06 UTC (606 KB)"
"34","Model-independent way to determine the Hubble constant and the curvature from phase shift of gravitational waves with DECIGO","Tonghua Liu, Shuo Cao, Marek Biesiada, Yilong Zhang, Jieci Wang","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In this Letter, we propose a model-independent method to determine the Hubble constant and curvature simultaneously taking advantage of the possibilities of future space-borne gravitational wave (GW) detector DECIGO in combination with the radio quasars as standard rulers. Similarly to the redshift drift in the electromagnetic domain, accelerating expansion of the Universe causes a characteristic phase correction to the gravitational waveform detectable by DECIGO. Hence, one would be able to extract the Hubble parameter $H(z)$. This could be used to recover distance-redshift relation supported by the data not relying on any specific cosmological model. Assuming the FLRW metric, and using intermediate luminosity radio quasars as standard rulers one achieves an interesting opportunity to directly assess $H_0$ and $\Omega_k$ parameters. To test this method we simulated a set of acceleration parameters achievable by future DECIGO. Based on the existing sample of 120 intermediate-luminosity radio-quasars calibrated as standard rulers, we simulated much bigger samples of such standard rulers possible to obtain with VLBI. In the case of $(N=100)$ of radio quasars, which is the size of currently available sample, the precision of cosmological parameters determined would be $\sigma_{H_0}=2.74$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ and $\sigma_{\Omega_k}=0.175$. In the optimistic scenario $(N = 1000)$ achievable by VLBI, the precision of $H_{0}$ would be improved to $1\%$, which is comparable to the result of $\sigma_{H_0} =0.54$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ from \emph{Planck} 2018 TT, TE, EE+lowE+lensing data, and the precision of $\Omega_k$ would be 0.050. Our results demonstrate that such combined analysis, possible in the future, could be helpful to solve the current cosmological issues concerning the Hubble tension and cosmic curvature tension.","Thu, 11 Apr 2024 01:21:20 UTC (232 KB)"
"35","Unveiling Behavioral Transparency of Protocols Communicated by IoT Networked Assets (Full Version)","Savindu Wannigama (1), Arunan Sivanathan (2), Ayyoob Hamza (2), Hassan Habibi Gharakheili (2) ((1) Department of Computer Engineering, University of Peradeniya, Sri Lanka. (2) School of EE&T, UNSW Sydney, Australia.)","Networking and Internet Architecture (cs.NI)","Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network. While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc. Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities. This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices. We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset. Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection. We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces.","Thu, 11 Apr 2024 00:47:14 UTC (88 KB)"
"36","Implementation of implicit filter for spatial spectra extraction","Kacper Nowak, Sergey Danilov, Vasco Müller, Caili Liu","Atmospheric and Oceanic Physics (physics.ao-ph)","Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis. It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations. However, for data from unstructured-mesh models it requires interpolation to a regular grid. We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians. This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models. The computation is split into two phases: preparation and solving. The first one is specific only to the mesh. This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time. The second part consists of sparse matrix algebra and solving linear system. Our implementation is accelerated by GPUs to achieve unmatched performance and scalability. This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds. As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations.","Thu, 11 Apr 2024 00:19:15 UTC (6,261 KB)"
"37","Kinematic age of the $β$-Pictoris moving group","Jinhee Lee, Inseok Song","Solar and Stellar Astrophysics (astro-ph.SR)","Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations. The $\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\sim$10 to 25 Myr, depending on the age estimation methods and data used. Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates. In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis. These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr. Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\sim2-4$ Myr. Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr. Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr. This age is significantly younger than the commonly accepted age of the BPMG ($\sim$24 Myr) determined primarily from the lithium depletion boundary analysis. This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method. This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups.","Wed, 10 Apr 2024 23:49:38 UTC (3,315 KB)"
"38","ALMA-IMF XV: N$_2$H$^+$ kinematic analysis on the intermediate protocluster G353.41","R. H. Álvarez-Gutiérrez, A. M. Stutz, N. Sandoval-Garrido, F. Louvet, F. Motte, R. Galván-Madrid, N. Cunningham, P. Sanhueza, M. Bonfand, S. Bontemps, A. Gusdorf, T. Csengeri, S. D. Reyes, J. Salinas, T. Baug, L. Bronfman, G. Busquet, D. J. Díaz-González, M. Fernandez-Lopez, A. Guzmán, A. Koley, H.-L. Liu, F. A. Olguin, M. Valeille-Manet, F. Wyrowski","Astrophysics of Galaxies (astro-ph.GA)","The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution. We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\times10^5$~cm$^{-3}$, and spatial resolution $\sim$0.02~pc. G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\sim$8~pc) filament and has a mass of 2500~M$_{\odot}$ within $1.3\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components. This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram. We identify multiple velocity gradients (VGs) on large and small scales. We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form. We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence. The average timescale associated with the V-shapes are $\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\sim$~0.5~Myr). We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates. This feeding might lead to further filament collapse and formation of new cores. We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall. Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime.","Wed, 10 Apr 2024 21:38:14 UTC (33,189 KB)"
"39","A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos","Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang","Computer Vision and Pattern Recognition (cs.CV)","Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.","Wed, 10 Apr 2024 21:14:33 UTC (2,567 KB)"
"40","Recovering the gas properties of protoplanetary disks through parametric visibility modeling: MHO 6","Nicolas T. Kurtovic, Paola Pinilla","Earth and Planetary Astrophysics (astro-ph.EP)","The composition and distribution of the gas in a protoplanetary disk plays a key role in shaping the outcome of the planet formation process. Observationally, the recovery of information such as the emission height and brightness temperature from interferometric data is often limited by the imaging processes. To overcome the limitations of image-reconstruction when analyzing gas emission from interferometric observations, we have introduced a parametric model to fit the main observable properties of the gaseous disk component in the visibility plane. This approach is also known as parametric visibility modeling. We applied our parametric visibility modeling to the gas brightness distribution of the molecular line emission from 12CO J=3-2 and 13CO J=3-2 in the disk around MHO 6, a very-low-mass star in the Taurus star-forming Region. To improve the flux fidelity of our parametric models, we combined models with different pixel resolution before the computation of their visibilities, referred to as ``nesting images.'' When we apply our parametric visibility modeling to MHO 6, with independent fits to the emission from its CO isopotologues, the models return the same consistent results for the stellar mass, disk geometry, and central velocity. The surface height and brightness temperature distribution are also recovered. When compared to other disks, MHO 6 surface height is among the most elevated surfaces, consistent with the predictions for disks around very-low-mass stars. This work demonstrates the feasibility of running rapidly iterable parametric visibility models in moderate resolution and sensitivity interferometric observations. More importantly, this methodology opens the analysis of disk's gas morphology to observations where image-based techniques are unable to robustly operate, as in the case of the compact disk around MHO 6.","Wed, 10 Apr 2024 21:08:34 UTC (1,768 KB)"
"41","Determination of $K^0_S$ Fragmentation Functions including BESIII Measurements and using Neural Networks","Maryam Soleymaninia, Hadi Hashamipour, Maral Salajegheh, Hamzeh Khanpour, Hubert Spiesberger, Ulf-G. Meißner","High Energy Physics - Phenomenology (hep-ph)","In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD. Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data. The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure. To address experimental uncertainties, the Monte Carlo method is employed. Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values. The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties.","Wed, 10 Apr 2024 20:27:33 UTC (10,974 KB)"
"42","An analysis of parameter compression and full-modeling techniques with Velocileptors for DESI 2024 and beyond","M. Maus, S. Chen, M. White, J. Aguilar, S. Ahlen, A. Aviles, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, C. Howlett, M. Ishak, S. Juneau, A. Kremin, Y. Lai, M. Landriau, M. E. Levi, M. Manera, R. Miquel, E. Mueller, A. D. Myers, S. Nadathur, J. Nie, H. E. Noriega, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, S. Ramirez-Solano, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, B. A. Weaver, S. Yuan, P. Zarrouk, H. Zhang, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In anticipation of forthcoming data releases of current and future spectroscopic surveys, we present the validation tests and analysis of systematic effects within \texttt{velocileptors} modeling pipeline when fitting mock data from the \texttt{AbacusSummit} N-body simulations. We compare the constraints obtained from parameter compression methods to the direct fitting (Full-Modeling) approaches of modeling the galaxy power spectra, and show that the ShapeFit extension to the traditional template method is consistent with the Full-Modeling method within the standard $\Lambda$CDM parameter space. We show the dependence on scale cuts when fitting the different redshift bins using the ShapeFit and Full-Modeling methods. We test the ability to jointly fit data from multiple redshift bins as well as joint analysis of the pre-reconstruction power spectrum with the post-reconstruction BAO correlation function signal. We further demonstrate the behavior of the model when opening up the parameter space beyond $\Lambda$CDM and also when combining likelihoods with external datasets, namely the Planck CMB priors. Finally, we describe different parametrization options for the galaxy bias, counterterm, and stochastic parameters, and employ the halo model in order to physically motivate suitable priors that are necessary to ensure the stability of the perturbation theory.","Wed, 10 Apr 2024 19:21:33 UTC (18,470 KB)"
"43","Effective uniaxial dielectric function tensor and optical phonons in ($\bar{2}01$)-plane oriented $β$-Ga$_2$O$_3$ films with equally-distributed six-fold rotation domains","Alyssa Mock, Steffen Richter, Alexis Papamichail, Vallery Stanishev, Misagh Ghezellou, Jawad Ul-Hassan, Andreas Popp, Saud Bin Anooz, Daniella Gogova, Praneeth Ranga, Sriram Krishnamoorthy, Rafal Korlacki, Mathias Schubert, Vanya Darakchieva","Materials Science (cond-mat.mtrl-sci)","Monoclinic $\beta$-Ga$_2$O$_3$ films grown on $c$-plane sapphire have been shown to exhibit six $(\bar{2}01)$-plane oriented domains, which are equally-spaced-by-rotation around the surface normal and equally-sized-by-volume that render the film optical response effectively uniaxial. We derive and discuss an optical model suitable for ellipsometry data analysis of such films. We model mid- and far-infrared ellipsometry data from undoped and electrically insulating films with an effective uniaxial dielectric tensor based on projections of all phonon modes within the rotation domains parallel and perpendicular to the sample normal, i.e., to the reciprocal lattice vector $\mathbf{g}_{\bar{2}01}$. Two effective response functions are described by model, and found sufficient to calculate ellipsometry data that best-match measured ellipsometry data from a representative film. We propose to render either effective dielectric functions, or inverse effective dielectric functions, each separately for electric field directions parallel and perpendicular to $\mathbf{g}_{\bar{2}01}$, by sums of Lorentz oscillators, which permit to determine either sets of transverse optical phonon mode parameters, or sets of longitudinal optical phonon mode parameters, respectively. Transverse optical modes common to both dielectric functions can be traced back to single crystal modes with $B_{\mathrm{u}}$ character, while modes with $A_{\mathrm{u}}$ character only appear within the dielectric function for polarization perpendicular to the sample surface. The thereby obtained parameter sets reveal all phonon modes anticipated from averaging over the six-fold rotation domains of single crystal $\beta$-Ga$_2$O$_3$, but with slightly shifted transverse optical, and completely different longitudinal optical phonon modes.","Wed, 10 Apr 2024 18:49:42 UTC (1,647 KB)"
"44","Quantifying the Errors Introduced by Continuum Scattering Models on the Inferred Structural Properties of Proteins","Rohan S. Adhikari, Dilipkumar N. Asthagiri, Walter G. Chapman","Chemical Physics (physics.chem-ph)","Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs). To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments. For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments. To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data. Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model. However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell. CRYSOL can over-predict Rg by up to 2.5 Angstroms. We rationalize the reason for this behavior and highlight the consequences for force field design.","Wed, 10 Apr 2024 18:36:37 UTC (5,947 KB)"
"45","A comparison between Shapefit compression and Full-Modelling method with PyBird for DESI 2024 and beyond","Yan Lai, Cullan Howlett, Mark Maus, Héctor Gil-Marín, Hernan E. Noriega, Sadi Ramírez-Solano, Pauline Zarrouk, Jessica N. Aguilar, Steven Ahlen, Otávio Alves, Alejandro Aviles, David Brooks, Shi-Fan Chen, Todd Claybaugh, Tamara M. Davis, Kyle Dawson, Axel de la Macorra, Peter Doel, Jaime E. Forero-Romero, Enrique Gaztañaga, Satya Gontcho A Gontcho, Klaus Honscheid, Stephanie Juneau, Martin Landriau, Marc Manera, Ramon Miquel, Eva-Maria Mueller, Seshadri Nadathur, Gustavo Niz, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Mehdi Rezaie, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, David Sprayberry, Gregory Tarlé, Mariana Vargas-Magaña, Licia Verde, Sihan Yuan, Rongpu Zhou, Hu Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","DESI aims to provide one of the tightest constraints on cosmological parameters by analyzing the clustering of more than thirty million galaxies. However, obtaining such constraints requires special care in validating the analysis methods, and efforts to reduce the computational time required through techniques such as data compression and emulation. In this work, we perform a precision validation of the PyBird power spectrum modelling code with both a traditional, but emulated, Full-Modelling approach and the model-independent Shapefit compression approach. Using cubic simulations, which accurately reproduce the clustering and precision of the DESI survey, we find that the cosmological constraints from Shapefit and Full-Modelling are consistent with each other at the $\sim0.3\sigma$ level. Both Shapefit and Full-Modelling are also consistent with the true $\Lambda$CDM simulation cosmology, even when including the hexadecapole, down to a scale $k_{\mathrm{max}} = 0.20 h \mathrm{Mpc}^{-1}$. For extended models such as the $w$CDM and the $o$CDM models, we find including the hexadecapole can significantly improve the constraints and reduce the systematic errors with the same $k_{\mathrm{max}}$. Furthermore, we also show that the constraints on cosmological parameters with the correlation function evaluated from PyBird down to $s_{\mathrm{min}} = 30 h^{-1} \mathrm{Mpc}$ are unbiased, and consistent with the constraints from the power spectrum.","Wed, 10 Apr 2024 18:26:16 UTC (18,726 KB)"
"46","Validating the Galaxy and Quasar Catalog-Level Blinding Scheme for the DESI 2024 analysis","U. Andrade, J. Mena-Fernández, H. Awan, A. J. Ross, S. Brieden, J. Pan, A. de Mattia, J. Aguilar, S. Ahlen, O. Alves, D. Brooks, E. Buckley-Geer, E. Chaussidon, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, J. Guy, C. Hahn, M. M. S Hanif, K. Honscheid, C. Howlett, D. Huterer, S. Juneau, A. Kremin, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. A. Newman, J. Nie, G. Niz, N. Palanque-Delabrouille, W. J. Percival, M. Pinon, C. Poppett, F. Prada, M. Rashkovetskyi, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, L. Verde, B. A. Weaver","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the era of precision cosmology, ensuring the integrity of data analysis through blinding techniques is paramount -- a challenge particularly relevant for the Dark Energy Spectroscopic Instrument (DESI). DESI represents a monumental effort to map the cosmic web, with the goal to measure the redshifts of tens of millions of galaxies and quasars. Given the data volume and the impact of the findings, the potential for confirmation bias poses a significant challenge. To address this, we implement and validate a comprehensive blind analysis strategy for DESI Data Release 1 (DR1), tailored to the specific observables DESI is most sensitive to: Baryonic Acoustic Oscillations (BAO), Redshift-Space Distortion (RSD) and primordial non-Gaussianities (PNG). We carry out the blinding at the catalog level, implementing shifts in the redshifts of the observed galaxies to blind for BAO and RSD signals and weights to blind for PNG through a scale-dependent bias. We validate the blinding technique on mocks, as well as on data by applying a second blinding layer to perform a battery of sanity checks. We find that the blinding strategy alters the data vector in a controlled way such that the BAO and RSD analysis choices do not need any modification before and after unblinding. The successful validation of the blinding strategy paves the way for the unblinded DESI DR1 analysis, alongside future blind analyses with DESI and other surveys.","Wed, 10 Apr 2024 18:24:08 UTC (8,180 KB)"
"47","A comparison of effective field theory models of redshift space galaxy power spectra for DESI 2024 and future surveys","M. Maus, Y. Lai, H. E. Noriega, S. Ramirez-Solano, A. Aviles, S. Chen, S. Fromenteau, H. Gil-Marín, C. Howlett, M. Vargas-Magaña, M. White, P. Zarrouk, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, E. Burtin, T. Claybaugh, S. Cole, K. Dawson, M. Icaza-Lizaola, A. de la Macorra, A. de Mattia, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, M. Ishak, A. Kremin, M. Landriau, L. Le Guillou, M. Manera, R. Miquel, E. Mueller, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, S. Yuan, R. Zhao, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In preparation for the next generation of galaxy redshift surveys, and in particular the year-one data release from the Dark Energy Spectroscopic Instrument (DESI), we investigate the consistency of a variety of effective field theory models that describe the galaxy-galaxy power spectra in redshift space into the quasi-linear regime using 1-loop perturbation theory. These models are employed in the pipelines \texttt{velocileptors}, \texttt{PyBird}, and \texttt{Folps$\nu$}. While these models have been validated independently, a detailed comparison with consistent choices has not been attempted. After briefly discussing the theoretical differences between the models we describe how to provide a more apples-to-apples comparison between them. We present the results of fitting mock spectra from the \texttt{AbacusSummit} suite of N-body simulations provided in three redshift bins to mimic the types of dark time tracers targeted by the DESI survey. We show that the theories behave similarly and give consistent constraints in both the forward-modeling and ShapeFit compressed fitting approaches. We additionally generate (noiseless) synthetic data from each pipeline to be fit by the others, varying the scale cuts in order to show that the models agree within the range of scales for which we expect 1-loop perturbation theory to be applicable. This work lays the foundation of Full-Shape analysis with DESI Y1 galaxy samples where in the tests we performed, we found no systematic error associated with the modeling of the galaxy redshift space power spectrum for this volume.","Wed, 10 Apr 2024 18:03:38 UTC (8,310 KB)"
"48","Comparing Compressed and Full-modeling Analyses with FOLPS: Implications for DESI 2024 and beyond","H. E. Noriega, A. Aviles, H. Gil-Marín, S. Ramirez-Solano, S. Fromenteau, M. Vargas-Magaña, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, J. L. Cervantes-Cota, S. Chen, T. Claybaugh, S. Cole, K. Dawson, A. de la Macorra, A. de Mattia, P. Doel, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, K. Honscheid, J. Hou, C. Howlett, M. Ishak, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, G. Morales-Navarrete, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, L. Verde, S. Yuan, P. Zarrouk, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe. In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos. We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations. We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales. Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy. In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis. Furthermore, we show how opening up the parameter space beyond the vanilla $\Lambda$CDM model affects the DESI observables. These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state. We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest. This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS.","Wed, 10 Apr 2024 18:00:54 UTC (12,699 KB)"
"49","Full Modeling and Parameter Compression Methods in configuration space for DESI 2024 and beyond","S. Ramirez-Solano, M. Icaza-Lizaola, H. E. Noriega, M. Vargas-Magaña, S. Fromenteau, A. Aviles, F. Rodriguez-Martinez, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, B. Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, K. Honscheid, C. Howlett, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. Nie, W. J. Percival, C. Poppett, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, L. Verde, B. A. Weaver, R. H. Wechsler, S. Yuan, P. Zarrouk, H. Zou (DESI Collaboration)","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data. This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space. We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard). We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements. Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory. We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model. Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\sigma$ in all cases for a 1-year DESI volume. Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers. We find good agreement with the results from all these models.","Wed, 10 Apr 2024 18:00:53 UTC (8,730 KB)"
"50","Complete Optimal Non-Resonant Anomaly Detection","Gregor Kasieczka, John Andrew Raine, David Shih, Aman Upadhyay","High Energy Physics - Phenomenology (hep-ph)","We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions. Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature. The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features. Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses. The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\nu \nu) +\text{jets}$, and the data-driven control process is $\gamma+\text{jets}$.","Wed, 10 Apr 2024 18:00:01 UTC (859 KB)"
"51","Language Imbalance Can Boost Cross-lingual Generalisation","Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag","Computation and Language (cs.CL)","Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","Thu, 11 Apr 2024 17:58:05 UTC (4,890 KB)"
"52","OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","Artificial Intelligence (cs.AI)","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)"
"53","An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting","Chufeng Li, Jianyong Chen","Statistical Finance (q-fin.ST)","As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available at this https URL.","Mon, 25 Mar 2024 15:23:22 UTC (1,088 KB)"
"54","Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance","Xiangpeng Li, Ali Mostafavi","Computers and Society (cs.CY)","There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.","Sun, 24 Mar 2024 19:32:23 UTC (2,889 KB)"
"55","Visualization for physics analysis improvement and applications in BESIII","Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao","Data Analysis, Statistics and Probability (physics.data-an)","Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.","Tue, 19 Mar 2024 10:26:48 UTC (1,990 KB)"
"56","Distributed Record Linkage in Healthcare Data with Apache Spark","Mohammad Heydari, Reza Sarshar, Mohammad Ali Soltanshahi","Distributed, Parallel, and Cluster Computing (cs.DC)","Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.","Sat, 9 Mar 2024 05:18:15 UTC (249 KB)"
"57","Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","Charis Stamouli, Ingvar Ziemann, George J. Pappas","Statistics Theory (math.ST)","We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","Thu, 11 Apr 2024 17:36:28 UTC (45 KB)"
"58","Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","Tuhin Malik, V. Dexheimer, Constança Providência","Nuclear Theory (nucl-th)","We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\omega^2\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","Thu, 11 Apr 2024 17:35:54 UTC (1,088 KB)"
"59","Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","Kamal Aghazade, Ali Gholami, Hossein S. Aghamiry, Hamid Reza Siahkoohi","Numerical Analysis (math.NA)","Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","Thu, 11 Apr 2024 17:25:23 UTC (16,221 KB)"
"60","Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld","Human-Computer Interaction (cs.HC)","Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","Thu, 11 Apr 2024 16:40:24 UTC (1,038 KB)"
"61","Revisiting a drag partition model for canopy-like roughness elements","Elia Buono, Gabriel G. Katul, Davide Vettori, Davide Poggi, Costantino Manes","Fluid Dynamics (physics.flu-dyn)","Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","Thu, 11 Apr 2024 16:27:43 UTC (359 KB)"
"62","Diagram Analysis of Iterative Algorithms","Chris Jones, Lucas Pesenti","Computational Complexity (cs.CC)","We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.
We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.
The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by ""implementing"" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.
These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","Thu, 11 Apr 2024 16:10:52 UTC (1,097 KB)"
"63","The Power of Properties: Uncovering the Influential Factors in Emotion Classification","Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler","Computer Vision and Pattern Recognition (cs.CV)","Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","Thu, 11 Apr 2024 16:01:00 UTC (895 KB)"
"64","Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","Geng Chen, Faris A. El-Katri, Yanbo Hu, Yannan Shen","Analysis of PDEs (math.AP)","In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","Thu, 11 Apr 2024 15:15:35 UTC (34 KB)"
"65","iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang","Systems and Control (eess.SY)","This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","Thu, 11 Apr 2024 15:10:10 UTC (898 KB)"
"66","Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","Benjamin Lieberman, Andreas Crivellin, Salah-Eddine Dahbi, Finn Stevenson, Nidhi Tripathi, Mukesh Kumar, Bruce Mellado","High Energy Physics - Phenomenology (hep-ph)","To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","Thu, 11 Apr 2024 15:06:15 UTC (2,225 KB)"
"67","The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu","Analysis of PDEs (math.AP)","We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.
The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","Thu, 11 Apr 2024 14:52:25 UTC (70 KB)"
"68","Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann","Computer Vision and Pattern Recognition (cs.CV)","Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","Thu, 11 Apr 2024 14:00:20 UTC (23,169 KB)"
"69","3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","Xuanming Cao, Chengyu Tao, Juan Du","Computer Vision and Pattern Recognition (cs.CV)","The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","Thu, 11 Apr 2024 13:46:05 UTC (6,818 KB)"
"70","Depth Estimation using Weighted-loss and Transfer Learning","Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah","Computer Vision and Pattern Recognition (cs.CV)","Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","Thu, 11 Apr 2024 12:25:54 UTC (2,233 KB)"
"71","Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman","Computer Vision and Pattern Recognition (cs.CV)","Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","Thu, 11 Apr 2024 12:24:47 UTC (10,043 KB)"
"72","Merger Analysis with Latent Price","Paul Koh","Econometrics (econ.EM)","Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).","Thu, 11 Apr 2024 12:23:57 UTC (43 KB)"
"73","Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","Marc Hallin, Simos G. Meintanis, Klaus Nordhausen","Methodology (stat.ME)","We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","Thu, 11 Apr 2024 10:48:23 UTC (593 KB)"
"74","Lower semicontinuity and existence results for anisotropic TV functionals with signed measure data","Eleonora Ficola, Thomas Schmidt","Analysis of PDEs (math.AP)","We study the minimization of anisotropic total variation functionals with additional measure terms among functions of bounded variation subject to a Dirichlet boundary condition. More specifically, we identify and characterize certain isoperimetric conditions, which prove to be sharp assumptions on the signed measure data in connection with semicontinuity, existence, and relaxation results. Furthermore, we present a variety of examples which elucidate our assumptions and results.","Thu, 11 Apr 2024 10:44:07 UTC (270 KB)"
"75","State-Space Modeling of Shape-constrained Functional Time Series","Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa","Applications (stat.AP)","Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.","Thu, 11 Apr 2024 09:17:30 UTC (1,484 KB)"
"76","M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu","Information Retrieval (cs.IR)","We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","Thu, 11 Apr 2024 09:13:52 UTC (2,290 KB)"
"77","GAN-based iterative motion estimation in HASTE MRI","Mathias S. Feinler, Bernadette N. Hahn","Numerical Analysis (math.NA)","Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","Thu, 11 Apr 2024 09:07:57 UTC (6,372 KB)"
"78","Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","Alessandro Candido, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo","High Energy Physics - Phenomenology (hep-ph)","We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","Thu, 11 Apr 2024 09:03:52 UTC (442 KB)"
"79","Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","Claudia Toci, Simone Ceppi, Nicolás Cuello, Gaspard Duchêne, Enrico Ragusa, Giuseppe Lodato, Francesca Farina, François Ménard, Hossam Aly","Earth and Planetary Astrophysics (astro-ph.EP)","GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\mu$m ALMA dust continuum emission and 1.67 $\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\Delta\theta= 30^\circ$) on timescales shorter than $\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","Thu, 11 Apr 2024 08:51:50 UTC (9,675 KB)"
"80","Language Imbalance Can Boost Cross-lingual Generalisation","Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag","Computation and Language (cs.CL)","Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","Thu, 11 Apr 2024 17:58:05 UTC (4,890 KB)"
"81","OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","Artificial Intelligence (cs.AI)","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)"
"82","An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting","Chufeng Li, Jianyong Chen","Statistical Finance (q-fin.ST)","As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available at this https URL.","Mon, 25 Mar 2024 15:23:22 UTC (1,088 KB)"
"83","Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance","Xiangpeng Li, Ali Mostafavi","Computers and Society (cs.CY)","There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.","Sun, 24 Mar 2024 19:32:23 UTC (2,889 KB)"
"84","Visualization for physics analysis improvement and applications in BESIII","Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao","Data Analysis, Statistics and Probability (physics.data-an)","Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.","Tue, 19 Mar 2024 10:26:48 UTC (1,990 KB)"
"85","Distributed Record Linkage in Healthcare Data with Apache Spark","Mohammad Heydari, Reza Sarshar, Mohammad Ali Soltanshahi","Distributed, Parallel, and Cluster Computing (cs.DC)","Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.","Sat, 9 Mar 2024 05:18:15 UTC (249 KB)"
"86","Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","Charis Stamouli, Ingvar Ziemann, George J. Pappas","Statistics Theory (math.ST)","We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","Thu, 11 Apr 2024 17:36:28 UTC (45 KB)"
"87","Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","Tuhin Malik, V. Dexheimer, Constança Providência","Nuclear Theory (nucl-th)","We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\omega^2\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","Thu, 11 Apr 2024 17:35:54 UTC (1,088 KB)"
"88","Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","Kamal Aghazade, Ali Gholami, Hossein S. Aghamiry, Hamid Reza Siahkoohi","Numerical Analysis (math.NA)","Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","Thu, 11 Apr 2024 17:25:23 UTC (16,221 KB)"
"89","Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld","Human-Computer Interaction (cs.HC)","Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","Thu, 11 Apr 2024 16:40:24 UTC (1,038 KB)"
"90","Revisiting a drag partition model for canopy-like roughness elements","Elia Buono, Gabriel G. Katul, Davide Vettori, Davide Poggi, Costantino Manes","Fluid Dynamics (physics.flu-dyn)","Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","Thu, 11 Apr 2024 16:27:43 UTC (359 KB)"
"91","Diagram Analysis of Iterative Algorithms","Chris Jones, Lucas Pesenti","Computational Complexity (cs.CC)","We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.
We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.
The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by ""implementing"" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.
These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","Thu, 11 Apr 2024 16:10:52 UTC (1,097 KB)"
"92","The Power of Properties: Uncovering the Influential Factors in Emotion Classification","Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler","Computer Vision and Pattern Recognition (cs.CV)","Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","Thu, 11 Apr 2024 16:01:00 UTC (895 KB)"
"93","Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","Geng Chen, Faris A. El-Katri, Yanbo Hu, Yannan Shen","Analysis of PDEs (math.AP)","In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","Thu, 11 Apr 2024 15:15:35 UTC (34 KB)"
"94","iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang","Systems and Control (eess.SY)","This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","Thu, 11 Apr 2024 15:10:10 UTC (898 KB)"
"95","Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","Benjamin Lieberman, Andreas Crivellin, Salah-Eddine Dahbi, Finn Stevenson, Nidhi Tripathi, Mukesh Kumar, Bruce Mellado","High Energy Physics - Phenomenology (hep-ph)","To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","Thu, 11 Apr 2024 15:06:15 UTC (2,225 KB)"
"96","The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu","Analysis of PDEs (math.AP)","We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.
The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","Thu, 11 Apr 2024 14:52:25 UTC (70 KB)"
"97","Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann","Computer Vision and Pattern Recognition (cs.CV)","Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","Thu, 11 Apr 2024 14:00:20 UTC (23,169 KB)"
"98","3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","Xuanming Cao, Chengyu Tao, Juan Du","Computer Vision and Pattern Recognition (cs.CV)","The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","Thu, 11 Apr 2024 13:46:05 UTC (6,818 KB)"
"99","Depth Estimation using Weighted-loss and Transfer Learning","Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah","Computer Vision and Pattern Recognition (cs.CV)","Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","Thu, 11 Apr 2024 12:25:54 UTC (2,233 KB)"
"100","Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman","Computer Vision and Pattern Recognition (cs.CV)","Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","Thu, 11 Apr 2024 12:24:47 UTC (10,043 KB)"
"101","Merger Analysis with Latent Price","Paul Koh","Econometrics (econ.EM)","Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).","Thu, 11 Apr 2024 12:23:57 UTC (43 KB)"
"102","Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","Marc Hallin, Simos G. Meintanis, Klaus Nordhausen","Methodology (stat.ME)","We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","Thu, 11 Apr 2024 10:48:23 UTC (593 KB)"
"103","Lower semicontinuity and existence results for anisotropic TV functionals with signed measure data","Eleonora Ficola, Thomas Schmidt","Analysis of PDEs (math.AP)","We study the minimization of anisotropic total variation functionals with additional measure terms among functions of bounded variation subject to a Dirichlet boundary condition. More specifically, we identify and characterize certain isoperimetric conditions, which prove to be sharp assumptions on the signed measure data in connection with semicontinuity, existence, and relaxation results. Furthermore, we present a variety of examples which elucidate our assumptions and results.","Thu, 11 Apr 2024 10:44:07 UTC (270 KB)"
"104","State-Space Modeling of Shape-constrained Functional Time Series","Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa","Applications (stat.AP)","Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.","Thu, 11 Apr 2024 09:17:30 UTC (1,484 KB)"
"105","M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu","Information Retrieval (cs.IR)","We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","Thu, 11 Apr 2024 09:13:52 UTC (2,290 KB)"
"106","GAN-based iterative motion estimation in HASTE MRI","Mathias S. Feinler, Bernadette N. Hahn","Numerical Analysis (math.NA)","Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","Thu, 11 Apr 2024 09:07:57 UTC (6,372 KB)"
"107","Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","Alessandro Candido, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo","High Energy Physics - Phenomenology (hep-ph)","We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","Thu, 11 Apr 2024 09:03:52 UTC (442 KB)"
"108","Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","Claudia Toci, Simone Ceppi, Nicolás Cuello, Gaspard Duchêne, Enrico Ragusa, Giuseppe Lodato, Francesca Farina, François Ménard, Hossam Aly","Earth and Planetary Astrophysics (astro-ph.EP)","GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\mu$m ALMA dust continuum emission and 1.67 $\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\Delta\theta= 30^\circ$) on timescales shorter than $\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","Thu, 11 Apr 2024 08:51:50 UTC (9,675 KB)"
"109","X-ray polarimetric features of Gamma-ray Bursts across varied redshifts and hints for Axion-Like-Particles","Qingxiang Zhang, Feng Huang, Zhongxiang Wang, Taotao Fang","High Energy Astrophysical Phenomena (astro-ph.HE)","Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs. However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints. In this work, we first examine the statistical characteristics of linear polarization degree ($\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions. Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies. We then explore alternations to the initial $\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\bf B}_{\rm IGM} $). With the existence of ALPs with $m_{a}$$~$$\lesssim$$~$$10^{-14}$$~$eV and $g_{a\gamma}~$$\simeq$$~0.5\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons. To ensure that the effect of mixing is small enough to be negligible, the mixing term $\Delta_{a\gamma} \equiv 1/2\ g_{a\gamma} {\bf B}_{\rm IGM}$ should be less than $1.5\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited. Certification of redshift for GRBs with low $\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\bf B}_{\rm IGM}$.","Thu, 11 Apr 2024 08:36:21 UTC (2,354 KB)"
"110","How is Visual Attention Influenced by Text Guidance? Database and Model","Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai","Computer Vision and Pattern Recognition (cs.CV)","The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: this https URL.","Thu, 11 Apr 2024 08:03:23 UTC (3,122 KB)"
"111","On the convergence analysis of one-shot inversion methods","Marcella Bonazzoli (IDEFIX), Houssem Haddar (IDEFIX), Tuan Anh Vu (IDEFIX)","Numerical Analysis (math.NA)","When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively. The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods. We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions. Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem. We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter. We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations. Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead. We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data.","Thu, 11 Apr 2024 07:40:41 UTC (3,679 KB)"
"112","Characterizing the Influence of Topology on Graph Learning Tasks","Kailong Wu, Yule Xie, Jiaxin Ding, Yuxiang Ren, Luoyi Fu, Xinbing Wang, Chenghu Zhou","Machine Learning (cs.LG)","Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives. We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning.","Thu, 11 Apr 2024 06:04:06 UTC (606 KB)"
"113","Model-independent way to determine the Hubble constant and the curvature from phase shift of gravitational waves with DECIGO","Tonghua Liu, Shuo Cao, Marek Biesiada, Yilong Zhang, Jieci Wang","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In this Letter, we propose a model-independent method to determine the Hubble constant and curvature simultaneously taking advantage of the possibilities of future space-borne gravitational wave (GW) detector DECIGO in combination with the radio quasars as standard rulers. Similarly to the redshift drift in the electromagnetic domain, accelerating expansion of the Universe causes a characteristic phase correction to the gravitational waveform detectable by DECIGO. Hence, one would be able to extract the Hubble parameter $H(z)$. This could be used to recover distance-redshift relation supported by the data not relying on any specific cosmological model. Assuming the FLRW metric, and using intermediate luminosity radio quasars as standard rulers one achieves an interesting opportunity to directly assess $H_0$ and $\Omega_k$ parameters. To test this method we simulated a set of acceleration parameters achievable by future DECIGO. Based on the existing sample of 120 intermediate-luminosity radio-quasars calibrated as standard rulers, we simulated much bigger samples of such standard rulers possible to obtain with VLBI. In the case of $(N=100)$ of radio quasars, which is the size of currently available sample, the precision of cosmological parameters determined would be $\sigma_{H_0}=2.74$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ and $\sigma_{\Omega_k}=0.175$. In the optimistic scenario $(N = 1000)$ achievable by VLBI, the precision of $H_{0}$ would be improved to $1\%$, which is comparable to the result of $\sigma_{H_0} =0.54$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ from \emph{Planck} 2018 TT, TE, EE+lowE+lensing data, and the precision of $\Omega_k$ would be 0.050. Our results demonstrate that such combined analysis, possible in the future, could be helpful to solve the current cosmological issues concerning the Hubble tension and cosmic curvature tension.","Thu, 11 Apr 2024 01:21:20 UTC (232 KB)"
"114","Unveiling Behavioral Transparency of Protocols Communicated by IoT Networked Assets (Full Version)","Savindu Wannigama (1), Arunan Sivanathan (2), Ayyoob Hamza (2), Hassan Habibi Gharakheili (2) ((1) Department of Computer Engineering, University of Peradeniya, Sri Lanka. (2) School of EE&T, UNSW Sydney, Australia.)","Networking and Internet Architecture (cs.NI)","Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network. While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc. Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities. This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices. We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset. Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection. We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces.","Thu, 11 Apr 2024 00:47:14 UTC (88 KB)"
"115","Implementation of implicit filter for spatial spectra extraction","Kacper Nowak, Sergey Danilov, Vasco Müller, Caili Liu","Atmospheric and Oceanic Physics (physics.ao-ph)","Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis. It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations. However, for data from unstructured-mesh models it requires interpolation to a regular grid. We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians. This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models. The computation is split into two phases: preparation and solving. The first one is specific only to the mesh. This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time. The second part consists of sparse matrix algebra and solving linear system. Our implementation is accelerated by GPUs to achieve unmatched performance and scalability. This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds. As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations.","Thu, 11 Apr 2024 00:19:15 UTC (6,261 KB)"
"116","Kinematic age of the $β$-Pictoris moving group","Jinhee Lee, Inseok Song","Solar and Stellar Astrophysics (astro-ph.SR)","Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations. The $\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\sim$10 to 25 Myr, depending on the age estimation methods and data used. Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates. In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis. These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr. Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\sim2-4$ Myr. Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr. Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr. This age is significantly younger than the commonly accepted age of the BPMG ($\sim$24 Myr) determined primarily from the lithium depletion boundary analysis. This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method. This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups.","Wed, 10 Apr 2024 23:49:38 UTC (3,315 KB)"
"117","ALMA-IMF XV: N$_2$H$^+$ kinematic analysis on the intermediate protocluster G353.41","R. H. Álvarez-Gutiérrez, A. M. Stutz, N. Sandoval-Garrido, F. Louvet, F. Motte, R. Galván-Madrid, N. Cunningham, P. Sanhueza, M. Bonfand, S. Bontemps, A. Gusdorf, T. Csengeri, S. D. Reyes, J. Salinas, T. Baug, L. Bronfman, G. Busquet, D. J. Díaz-González, M. Fernandez-Lopez, A. Guzmán, A. Koley, H.-L. Liu, F. A. Olguin, M. Valeille-Manet, F. Wyrowski","Astrophysics of Galaxies (astro-ph.GA)","The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution. We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\times10^5$~cm$^{-3}$, and spatial resolution $\sim$0.02~pc. G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\sim$8~pc) filament and has a mass of 2500~M$_{\odot}$ within $1.3\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components. This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram. We identify multiple velocity gradients (VGs) on large and small scales. We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form. We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence. The average timescale associated with the V-shapes are $\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\sim$~0.5~Myr). We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates. This feeding might lead to further filament collapse and formation of new cores. We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall. Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime.","Wed, 10 Apr 2024 21:38:14 UTC (33,189 KB)"
"118","A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos","Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang","Computer Vision and Pattern Recognition (cs.CV)","Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.","Wed, 10 Apr 2024 21:14:33 UTC (2,567 KB)"
"119","Recovering the gas properties of protoplanetary disks through parametric visibility modeling: MHO 6","Nicolas T. Kurtovic, Paola Pinilla","Earth and Planetary Astrophysics (astro-ph.EP)","The composition and distribution of the gas in a protoplanetary disk plays a key role in shaping the outcome of the planet formation process. Observationally, the recovery of information such as the emission height and brightness temperature from interferometric data is often limited by the imaging processes. To overcome the limitations of image-reconstruction when analyzing gas emission from interferometric observations, we have introduced a parametric model to fit the main observable properties of the gaseous disk component in the visibility plane. This approach is also known as parametric visibility modeling. We applied our parametric visibility modeling to the gas brightness distribution of the molecular line emission from 12CO J=3-2 and 13CO J=3-2 in the disk around MHO 6, a very-low-mass star in the Taurus star-forming Region. To improve the flux fidelity of our parametric models, we combined models with different pixel resolution before the computation of their visibilities, referred to as ``nesting images.'' When we apply our parametric visibility modeling to MHO 6, with independent fits to the emission from its CO isopotologues, the models return the same consistent results for the stellar mass, disk geometry, and central velocity. The surface height and brightness temperature distribution are also recovered. When compared to other disks, MHO 6 surface height is among the most elevated surfaces, consistent with the predictions for disks around very-low-mass stars. This work demonstrates the feasibility of running rapidly iterable parametric visibility models in moderate resolution and sensitivity interferometric observations. More importantly, this methodology opens the analysis of disk's gas morphology to observations where image-based techniques are unable to robustly operate, as in the case of the compact disk around MHO 6.","Wed, 10 Apr 2024 21:08:34 UTC (1,768 KB)"
"120","Determination of $K^0_S$ Fragmentation Functions including BESIII Measurements and using Neural Networks","Maryam Soleymaninia, Hadi Hashamipour, Maral Salajegheh, Hamzeh Khanpour, Hubert Spiesberger, Ulf-G. Meißner","High Energy Physics - Phenomenology (hep-ph)","In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD. Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data. The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure. To address experimental uncertainties, the Monte Carlo method is employed. Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values. The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties.","Wed, 10 Apr 2024 20:27:33 UTC (10,974 KB)"
"121","An analysis of parameter compression and full-modeling techniques with Velocileptors for DESI 2024 and beyond","M. Maus, S. Chen, M. White, J. Aguilar, S. Ahlen, A. Aviles, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, C. Howlett, M. Ishak, S. Juneau, A. Kremin, Y. Lai, M. Landriau, M. E. Levi, M. Manera, R. Miquel, E. Mueller, A. D. Myers, S. Nadathur, J. Nie, H. E. Noriega, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, S. Ramirez-Solano, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, B. A. Weaver, S. Yuan, P. Zarrouk, H. Zhang, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In anticipation of forthcoming data releases of current and future spectroscopic surveys, we present the validation tests and analysis of systematic effects within \texttt{velocileptors} modeling pipeline when fitting mock data from the \texttt{AbacusSummit} N-body simulations. We compare the constraints obtained from parameter compression methods to the direct fitting (Full-Modeling) approaches of modeling the galaxy power spectra, and show that the ShapeFit extension to the traditional template method is consistent with the Full-Modeling method within the standard $\Lambda$CDM parameter space. We show the dependence on scale cuts when fitting the different redshift bins using the ShapeFit and Full-Modeling methods. We test the ability to jointly fit data from multiple redshift bins as well as joint analysis of the pre-reconstruction power spectrum with the post-reconstruction BAO correlation function signal. We further demonstrate the behavior of the model when opening up the parameter space beyond $\Lambda$CDM and also when combining likelihoods with external datasets, namely the Planck CMB priors. Finally, we describe different parametrization options for the galaxy bias, counterterm, and stochastic parameters, and employ the halo model in order to physically motivate suitable priors that are necessary to ensure the stability of the perturbation theory.","Wed, 10 Apr 2024 19:21:33 UTC (18,470 KB)"
"122","Effective uniaxial dielectric function tensor and optical phonons in ($\bar{2}01$)-plane oriented $β$-Ga$_2$O$_3$ films with equally-distributed six-fold rotation domains","Alyssa Mock, Steffen Richter, Alexis Papamichail, Vallery Stanishev, Misagh Ghezellou, Jawad Ul-Hassan, Andreas Popp, Saud Bin Anooz, Daniella Gogova, Praneeth Ranga, Sriram Krishnamoorthy, Rafal Korlacki, Mathias Schubert, Vanya Darakchieva","Materials Science (cond-mat.mtrl-sci)","Monoclinic $\beta$-Ga$_2$O$_3$ films grown on $c$-plane sapphire have been shown to exhibit six $(\bar{2}01)$-plane oriented domains, which are equally-spaced-by-rotation around the surface normal and equally-sized-by-volume that render the film optical response effectively uniaxial. We derive and discuss an optical model suitable for ellipsometry data analysis of such films. We model mid- and far-infrared ellipsometry data from undoped and electrically insulating films with an effective uniaxial dielectric tensor based on projections of all phonon modes within the rotation domains parallel and perpendicular to the sample normal, i.e., to the reciprocal lattice vector $\mathbf{g}_{\bar{2}01}$. Two effective response functions are described by model, and found sufficient to calculate ellipsometry data that best-match measured ellipsometry data from a representative film. We propose to render either effective dielectric functions, or inverse effective dielectric functions, each separately for electric field directions parallel and perpendicular to $\mathbf{g}_{\bar{2}01}$, by sums of Lorentz oscillators, which permit to determine either sets of transverse optical phonon mode parameters, or sets of longitudinal optical phonon mode parameters, respectively. Transverse optical modes common to both dielectric functions can be traced back to single crystal modes with $B_{\mathrm{u}}$ character, while modes with $A_{\mathrm{u}}$ character only appear within the dielectric function for polarization perpendicular to the sample surface. The thereby obtained parameter sets reveal all phonon modes anticipated from averaging over the six-fold rotation domains of single crystal $\beta$-Ga$_2$O$_3$, but with slightly shifted transverse optical, and completely different longitudinal optical phonon modes.","Wed, 10 Apr 2024 18:49:42 UTC (1,647 KB)"
"123","Quantifying the Errors Introduced by Continuum Scattering Models on the Inferred Structural Properties of Proteins","Rohan S. Adhikari, Dilipkumar N. Asthagiri, Walter G. Chapman","Chemical Physics (physics.chem-ph)","Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs). To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments. For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments. To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data. Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model. However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell. CRYSOL can over-predict Rg by up to 2.5 Angstroms. We rationalize the reason for this behavior and highlight the consequences for force field design.","Wed, 10 Apr 2024 18:36:37 UTC (5,947 KB)"
"124","A comparison between Shapefit compression and Full-Modelling method with PyBird for DESI 2024 and beyond","Yan Lai, Cullan Howlett, Mark Maus, Héctor Gil-Marín, Hernan E. Noriega, Sadi Ramírez-Solano, Pauline Zarrouk, Jessica N. Aguilar, Steven Ahlen, Otávio Alves, Alejandro Aviles, David Brooks, Shi-Fan Chen, Todd Claybaugh, Tamara M. Davis, Kyle Dawson, Axel de la Macorra, Peter Doel, Jaime E. Forero-Romero, Enrique Gaztañaga, Satya Gontcho A Gontcho, Klaus Honscheid, Stephanie Juneau, Martin Landriau, Marc Manera, Ramon Miquel, Eva-Maria Mueller, Seshadri Nadathur, Gustavo Niz, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Mehdi Rezaie, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, David Sprayberry, Gregory Tarlé, Mariana Vargas-Magaña, Licia Verde, Sihan Yuan, Rongpu Zhou, Hu Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","DESI aims to provide one of the tightest constraints on cosmological parameters by analyzing the clustering of more than thirty million galaxies. However, obtaining such constraints requires special care in validating the analysis methods, and efforts to reduce the computational time required through techniques such as data compression and emulation. In this work, we perform a precision validation of the PyBird power spectrum modelling code with both a traditional, but emulated, Full-Modelling approach and the model-independent Shapefit compression approach. Using cubic simulations, which accurately reproduce the clustering and precision of the DESI survey, we find that the cosmological constraints from Shapefit and Full-Modelling are consistent with each other at the $\sim0.3\sigma$ level. Both Shapefit and Full-Modelling are also consistent with the true $\Lambda$CDM simulation cosmology, even when including the hexadecapole, down to a scale $k_{\mathrm{max}} = 0.20 h \mathrm{Mpc}^{-1}$. For extended models such as the $w$CDM and the $o$CDM models, we find including the hexadecapole can significantly improve the constraints and reduce the systematic errors with the same $k_{\mathrm{max}}$. Furthermore, we also show that the constraints on cosmological parameters with the correlation function evaluated from PyBird down to $s_{\mathrm{min}} = 30 h^{-1} \mathrm{Mpc}$ are unbiased, and consistent with the constraints from the power spectrum.","Wed, 10 Apr 2024 18:26:16 UTC (18,726 KB)"
"125","Validating the Galaxy and Quasar Catalog-Level Blinding Scheme for the DESI 2024 analysis","U. Andrade, J. Mena-Fernández, H. Awan, A. J. Ross, S. Brieden, J. Pan, A. de Mattia, J. Aguilar, S. Ahlen, O. Alves, D. Brooks, E. Buckley-Geer, E. Chaussidon, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, J. Guy, C. Hahn, M. M. S Hanif, K. Honscheid, C. Howlett, D. Huterer, S. Juneau, A. Kremin, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. A. Newman, J. Nie, G. Niz, N. Palanque-Delabrouille, W. J. Percival, M. Pinon, C. Poppett, F. Prada, M. Rashkovetskyi, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, L. Verde, B. A. Weaver","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the era of precision cosmology, ensuring the integrity of data analysis through blinding techniques is paramount -- a challenge particularly relevant for the Dark Energy Spectroscopic Instrument (DESI). DESI represents a monumental effort to map the cosmic web, with the goal to measure the redshifts of tens of millions of galaxies and quasars. Given the data volume and the impact of the findings, the potential for confirmation bias poses a significant challenge. To address this, we implement and validate a comprehensive blind analysis strategy for DESI Data Release 1 (DR1), tailored to the specific observables DESI is most sensitive to: Baryonic Acoustic Oscillations (BAO), Redshift-Space Distortion (RSD) and primordial non-Gaussianities (PNG). We carry out the blinding at the catalog level, implementing shifts in the redshifts of the observed galaxies to blind for BAO and RSD signals and weights to blind for PNG through a scale-dependent bias. We validate the blinding technique on mocks, as well as on data by applying a second blinding layer to perform a battery of sanity checks. We find that the blinding strategy alters the data vector in a controlled way such that the BAO and RSD analysis choices do not need any modification before and after unblinding. The successful validation of the blinding strategy paves the way for the unblinded DESI DR1 analysis, alongside future blind analyses with DESI and other surveys.","Wed, 10 Apr 2024 18:24:08 UTC (8,180 KB)"
"126","A comparison of effective field theory models of redshift space galaxy power spectra for DESI 2024 and future surveys","M. Maus, Y. Lai, H. E. Noriega, S. Ramirez-Solano, A. Aviles, S. Chen, S. Fromenteau, H. Gil-Marín, C. Howlett, M. Vargas-Magaña, M. White, P. Zarrouk, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, E. Burtin, T. Claybaugh, S. Cole, K. Dawson, M. Icaza-Lizaola, A. de la Macorra, A. de Mattia, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, M. Ishak, A. Kremin, M. Landriau, L. Le Guillou, M. Manera, R. Miquel, E. Mueller, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, S. Yuan, R. Zhao, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In preparation for the next generation of galaxy redshift surveys, and in particular the year-one data release from the Dark Energy Spectroscopic Instrument (DESI), we investigate the consistency of a variety of effective field theory models that describe the galaxy-galaxy power spectra in redshift space into the quasi-linear regime using 1-loop perturbation theory. These models are employed in the pipelines \texttt{velocileptors}, \texttt{PyBird}, and \texttt{Folps$\nu$}. While these models have been validated independently, a detailed comparison with consistent choices has not been attempted. After briefly discussing the theoretical differences between the models we describe how to provide a more apples-to-apples comparison between them. We present the results of fitting mock spectra from the \texttt{AbacusSummit} suite of N-body simulations provided in three redshift bins to mimic the types of dark time tracers targeted by the DESI survey. We show that the theories behave similarly and give consistent constraints in both the forward-modeling and ShapeFit compressed fitting approaches. We additionally generate (noiseless) synthetic data from each pipeline to be fit by the others, varying the scale cuts in order to show that the models agree within the range of scales for which we expect 1-loop perturbation theory to be applicable. This work lays the foundation of Full-Shape analysis with DESI Y1 galaxy samples where in the tests we performed, we found no systematic error associated with the modeling of the galaxy redshift space power spectrum for this volume.","Wed, 10 Apr 2024 18:03:38 UTC (8,310 KB)"
"127","Comparing Compressed and Full-modeling Analyses with FOLPS: Implications for DESI 2024 and beyond","H. E. Noriega, A. Aviles, H. Gil-Marín, S. Ramirez-Solano, S. Fromenteau, M. Vargas-Magaña, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, J. L. Cervantes-Cota, S. Chen, T. Claybaugh, S. Cole, K. Dawson, A. de la Macorra, A. de Mattia, P. Doel, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, K. Honscheid, J. Hou, C. Howlett, M. Ishak, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, G. Morales-Navarrete, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, L. Verde, S. Yuan, P. Zarrouk, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe. In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos. We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations. We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales. Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy. In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis. Furthermore, we show how opening up the parameter space beyond the vanilla $\Lambda$CDM model affects the DESI observables. These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state. We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest. This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS.","Wed, 10 Apr 2024 18:00:54 UTC (12,699 KB)"
"128","Full Modeling and Parameter Compression Methods in configuration space for DESI 2024 and beyond","S. Ramirez-Solano, M. Icaza-Lizaola, H. E. Noriega, M. Vargas-Magaña, S. Fromenteau, A. Aviles, F. Rodriguez-Martinez, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, B. Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, K. Honscheid, C. Howlett, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. Nie, W. J. Percival, C. Poppett, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, L. Verde, B. A. Weaver, R. H. Wechsler, S. Yuan, P. Zarrouk, H. Zou (DESI Collaboration)","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data. This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space. We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard). We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements. Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory. We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model. Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\sigma$ in all cases for a 1-year DESI volume. Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers. We find good agreement with the results from all these models.","Wed, 10 Apr 2024 18:00:53 UTC (8,730 KB)"
"129","Complete Optimal Non-Resonant Anomaly Detection","Gregor Kasieczka, John Andrew Raine, David Shih, Aman Upadhyay","High Energy Physics - Phenomenology (hep-ph)","We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions. Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature. The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features. Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses. The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\nu \nu) +\text{jets}$, and the data-driven control process is $\gamma+\text{jets}$.","Wed, 10 Apr 2024 18:00:01 UTC (859 KB)"
"130","Language Imbalance Can Boost Cross-lingual Generalisation","Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag","Computation and Language (cs.CL)","Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","Thu, 11 Apr 2024 17:58:05 UTC (4,890 KB)"
"131","OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","Artificial Intelligence (cs.AI)","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)"
"132","An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting","Chufeng Li, Jianyong Chen","Statistical Finance (q-fin.ST)","As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available at this https URL.","Mon, 25 Mar 2024 15:23:22 UTC (1,088 KB)"
"133","Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance","Xiangpeng Li, Ali Mostafavi","Computers and Society (cs.CY)","There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.","Sun, 24 Mar 2024 19:32:23 UTC (2,889 KB)"
"134","Visualization for physics analysis improvement and applications in BESIII","Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao","Data Analysis, Statistics and Probability (physics.data-an)","Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.","Tue, 19 Mar 2024 10:26:48 UTC (1,990 KB)"
"135","Distributed Record Linkage in Healthcare Data with Apache Spark","Mohammad Heydari, Reza Sarshar, Mohammad Ali Soltanshahi","Distributed, Parallel, and Cluster Computing (cs.DC)","Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.","Sat, 9 Mar 2024 05:18:15 UTC (249 KB)"
"136","Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","Charis Stamouli, Ingvar Ziemann, George J. Pappas","Statistics Theory (math.ST)","We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","Thu, 11 Apr 2024 17:36:28 UTC (45 KB)"
"137","Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","Tuhin Malik, V. Dexheimer, Constança Providência","Nuclear Theory (nucl-th)","We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\omega^2\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","Thu, 11 Apr 2024 17:35:54 UTC (1,088 KB)"
"138","Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","Kamal Aghazade, Ali Gholami, Hossein S. Aghamiry, Hamid Reza Siahkoohi","Numerical Analysis (math.NA)","Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","Thu, 11 Apr 2024 17:25:23 UTC (16,221 KB)"
"139","Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld","Human-Computer Interaction (cs.HC)","Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","Thu, 11 Apr 2024 16:40:24 UTC (1,038 KB)"
"140","Revisiting a drag partition model for canopy-like roughness elements","Elia Buono, Gabriel G. Katul, Davide Vettori, Davide Poggi, Costantino Manes","Fluid Dynamics (physics.flu-dyn)","Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","Thu, 11 Apr 2024 16:27:43 UTC (359 KB)"
"141","Diagram Analysis of Iterative Algorithms","Chris Jones, Lucas Pesenti","Computational Complexity (cs.CC)","We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.
We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.
The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by ""implementing"" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.
These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","Thu, 11 Apr 2024 16:10:52 UTC (1,097 KB)"
"142","The Power of Properties: Uncovering the Influential Factors in Emotion Classification","Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler","Computer Vision and Pattern Recognition (cs.CV)","Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","Thu, 11 Apr 2024 16:01:00 UTC (895 KB)"
"143","Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","Geng Chen, Faris A. El-Katri, Yanbo Hu, Yannan Shen","Analysis of PDEs (math.AP)","In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","Thu, 11 Apr 2024 15:15:35 UTC (34 KB)"
"144","iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang","Systems and Control (eess.SY)","This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","Thu, 11 Apr 2024 15:10:10 UTC (898 KB)"
"145","Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","Benjamin Lieberman, Andreas Crivellin, Salah-Eddine Dahbi, Finn Stevenson, Nidhi Tripathi, Mukesh Kumar, Bruce Mellado","High Energy Physics - Phenomenology (hep-ph)","To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","Thu, 11 Apr 2024 15:06:15 UTC (2,225 KB)"
"146","The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu","Analysis of PDEs (math.AP)","We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.
The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","Thu, 11 Apr 2024 14:52:25 UTC (70 KB)"
"147","Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann","Computer Vision and Pattern Recognition (cs.CV)","Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","Thu, 11 Apr 2024 14:00:20 UTC (23,169 KB)"
"148","3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","Xuanming Cao, Chengyu Tao, Juan Du","Computer Vision and Pattern Recognition (cs.CV)","The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","Thu, 11 Apr 2024 13:46:05 UTC (6,818 KB)"
"149","Depth Estimation using Weighted-loss and Transfer Learning","Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah","Computer Vision and Pattern Recognition (cs.CV)","Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","Thu, 11 Apr 2024 12:25:54 UTC (2,233 KB)"
"150","Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman","Computer Vision and Pattern Recognition (cs.CV)","Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","Thu, 11 Apr 2024 12:24:47 UTC (10,043 KB)"
"151","Merger Analysis with Latent Price","Paul Koh","Econometrics (econ.EM)","Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).","Thu, 11 Apr 2024 12:23:57 UTC (43 KB)"
"152","Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","Marc Hallin, Simos G. Meintanis, Klaus Nordhausen","Methodology (stat.ME)","We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","Thu, 11 Apr 2024 10:48:23 UTC (593 KB)"
"153","Lower semicontinuity and existence results for anisotropic TV functionals with signed measure data","Eleonora Ficola, Thomas Schmidt","Analysis of PDEs (math.AP)","We study the minimization of anisotropic total variation functionals with additional measure terms among functions of bounded variation subject to a Dirichlet boundary condition. More specifically, we identify and characterize certain isoperimetric conditions, which prove to be sharp assumptions on the signed measure data in connection with semicontinuity, existence, and relaxation results. Furthermore, we present a variety of examples which elucidate our assumptions and results.","Thu, 11 Apr 2024 10:44:07 UTC (270 KB)"
"154","State-Space Modeling of Shape-constrained Functional Time Series","Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa","Applications (stat.AP)","Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.","Thu, 11 Apr 2024 09:17:30 UTC (1,484 KB)"
"155","M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu","Information Retrieval (cs.IR)","We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","Thu, 11 Apr 2024 09:13:52 UTC (2,290 KB)"
"156","GAN-based iterative motion estimation in HASTE MRI","Mathias S. Feinler, Bernadette N. Hahn","Numerical Analysis (math.NA)","Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","Thu, 11 Apr 2024 09:07:57 UTC (6,372 KB)"
"157","Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","Alessandro Candido, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo","High Energy Physics - Phenomenology (hep-ph)","We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","Thu, 11 Apr 2024 09:03:52 UTC (442 KB)"
"158","Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","Claudia Toci, Simone Ceppi, Nicolás Cuello, Gaspard Duchêne, Enrico Ragusa, Giuseppe Lodato, Francesca Farina, François Ménard, Hossam Aly","Earth and Planetary Astrophysics (astro-ph.EP)","GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\mu$m ALMA dust continuum emission and 1.67 $\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\Delta\theta= 30^\circ$) on timescales shorter than $\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","Thu, 11 Apr 2024 08:51:50 UTC (9,675 KB)"
"159","X-ray polarimetric features of Gamma-ray Bursts across varied redshifts and hints for Axion-Like-Particles","Qingxiang Zhang, Feng Huang, Zhongxiang Wang, Taotao Fang","High Energy Astrophysical Phenomena (astro-ph.HE)","Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs. However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints. In this work, we first examine the statistical characteristics of linear polarization degree ($\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions. Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies. We then explore alternations to the initial $\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\bf B}_{\rm IGM} $). With the existence of ALPs with $m_{a}$$~$$\lesssim$$~$$10^{-14}$$~$eV and $g_{a\gamma}~$$\simeq$$~0.5\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons. To ensure that the effect of mixing is small enough to be negligible, the mixing term $\Delta_{a\gamma} \equiv 1/2\ g_{a\gamma} {\bf B}_{\rm IGM}$ should be less than $1.5\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited. Certification of redshift for GRBs with low $\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\bf B}_{\rm IGM}$.","Thu, 11 Apr 2024 08:36:21 UTC (2,354 KB)"
"160","How is Visual Attention Influenced by Text Guidance? Database and Model","Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai","Computer Vision and Pattern Recognition (cs.CV)","The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: this https URL.","Thu, 11 Apr 2024 08:03:23 UTC (3,122 KB)"
"161","On the convergence analysis of one-shot inversion methods","Marcella Bonazzoli (IDEFIX), Houssem Haddar (IDEFIX), Tuan Anh Vu (IDEFIX)","Numerical Analysis (math.NA)","When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively. The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods. We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions. Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem. We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter. We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations. Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead. We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data.","Thu, 11 Apr 2024 07:40:41 UTC (3,679 KB)"
"162","Characterizing the Influence of Topology on Graph Learning Tasks","Kailong Wu, Yule Xie, Jiaxin Ding, Yuxiang Ren, Luoyi Fu, Xinbing Wang, Chenghu Zhou","Machine Learning (cs.LG)","Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives. We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning.","Thu, 11 Apr 2024 06:04:06 UTC (606 KB)"
"163","Model-independent way to determine the Hubble constant and the curvature from phase shift of gravitational waves with DECIGO","Tonghua Liu, Shuo Cao, Marek Biesiada, Yilong Zhang, Jieci Wang","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In this Letter, we propose a model-independent method to determine the Hubble constant and curvature simultaneously taking advantage of the possibilities of future space-borne gravitational wave (GW) detector DECIGO in combination with the radio quasars as standard rulers. Similarly to the redshift drift in the electromagnetic domain, accelerating expansion of the Universe causes a characteristic phase correction to the gravitational waveform detectable by DECIGO. Hence, one would be able to extract the Hubble parameter $H(z)$. This could be used to recover distance-redshift relation supported by the data not relying on any specific cosmological model. Assuming the FLRW metric, and using intermediate luminosity radio quasars as standard rulers one achieves an interesting opportunity to directly assess $H_0$ and $\Omega_k$ parameters. To test this method we simulated a set of acceleration parameters achievable by future DECIGO. Based on the existing sample of 120 intermediate-luminosity radio-quasars calibrated as standard rulers, we simulated much bigger samples of such standard rulers possible to obtain with VLBI. In the case of $(N=100)$ of radio quasars, which is the size of currently available sample, the precision of cosmological parameters determined would be $\sigma_{H_0}=2.74$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ and $\sigma_{\Omega_k}=0.175$. In the optimistic scenario $(N = 1000)$ achievable by VLBI, the precision of $H_{0}$ would be improved to $1\%$, which is comparable to the result of $\sigma_{H_0} =0.54$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ from \emph{Planck} 2018 TT, TE, EE+lowE+lensing data, and the precision of $\Omega_k$ would be 0.050. Our results demonstrate that such combined analysis, possible in the future, could be helpful to solve the current cosmological issues concerning the Hubble tension and cosmic curvature tension.","Thu, 11 Apr 2024 01:21:20 UTC (232 KB)"
"164","Unveiling Behavioral Transparency of Protocols Communicated by IoT Networked Assets (Full Version)","Savindu Wannigama (1), Arunan Sivanathan (2), Ayyoob Hamza (2), Hassan Habibi Gharakheili (2) ((1) Department of Computer Engineering, University of Peradeniya, Sri Lanka. (2) School of EE&T, UNSW Sydney, Australia.)","Networking and Internet Architecture (cs.NI)","Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network. While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc. Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities. This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices. We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset. Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection. We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces.","Thu, 11 Apr 2024 00:47:14 UTC (88 KB)"
"165","Implementation of implicit filter for spatial spectra extraction","Kacper Nowak, Sergey Danilov, Vasco Müller, Caili Liu","Atmospheric and Oceanic Physics (physics.ao-ph)","Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis. It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations. However, for data from unstructured-mesh models it requires interpolation to a regular grid. We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians. This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models. The computation is split into two phases: preparation and solving. The first one is specific only to the mesh. This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time. The second part consists of sparse matrix algebra and solving linear system. Our implementation is accelerated by GPUs to achieve unmatched performance and scalability. This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds. As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations.","Thu, 11 Apr 2024 00:19:15 UTC (6,261 KB)"
"166","Kinematic age of the $β$-Pictoris moving group","Jinhee Lee, Inseok Song","Solar and Stellar Astrophysics (astro-ph.SR)","Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations. The $\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\sim$10 to 25 Myr, depending on the age estimation methods and data used. Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates. In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis. These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr. Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\sim2-4$ Myr. Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr. Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr. This age is significantly younger than the commonly accepted age of the BPMG ($\sim$24 Myr) determined primarily from the lithium depletion boundary analysis. This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method. This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups.","Wed, 10 Apr 2024 23:49:38 UTC (3,315 KB)"
"167","ALMA-IMF XV: N$_2$H$^+$ kinematic analysis on the intermediate protocluster G353.41","R. H. Álvarez-Gutiérrez, A. M. Stutz, N. Sandoval-Garrido, F. Louvet, F. Motte, R. Galván-Madrid, N. Cunningham, P. Sanhueza, M. Bonfand, S. Bontemps, A. Gusdorf, T. Csengeri, S. D. Reyes, J. Salinas, T. Baug, L. Bronfman, G. Busquet, D. J. Díaz-González, M. Fernandez-Lopez, A. Guzmán, A. Koley, H.-L. Liu, F. A. Olguin, M. Valeille-Manet, F. Wyrowski","Astrophysics of Galaxies (astro-ph.GA)","The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution. We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\times10^5$~cm$^{-3}$, and spatial resolution $\sim$0.02~pc. G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\sim$8~pc) filament and has a mass of 2500~M$_{\odot}$ within $1.3\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components. This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram. We identify multiple velocity gradients (VGs) on large and small scales. We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form. We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence. The average timescale associated with the V-shapes are $\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\sim$~0.5~Myr). We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates. This feeding might lead to further filament collapse and formation of new cores. We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall. Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime.","Wed, 10 Apr 2024 21:38:14 UTC (33,189 KB)"
"168","A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos","Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang","Computer Vision and Pattern Recognition (cs.CV)","Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.","Wed, 10 Apr 2024 21:14:33 UTC (2,567 KB)"
"169","Recovering the gas properties of protoplanetary disks through parametric visibility modeling: MHO 6","Nicolas T. Kurtovic, Paola Pinilla","Earth and Planetary Astrophysics (astro-ph.EP)","The composition and distribution of the gas in a protoplanetary disk plays a key role in shaping the outcome of the planet formation process. Observationally, the recovery of information such as the emission height and brightness temperature from interferometric data is often limited by the imaging processes. To overcome the limitations of image-reconstruction when analyzing gas emission from interferometric observations, we have introduced a parametric model to fit the main observable properties of the gaseous disk component in the visibility plane. This approach is also known as parametric visibility modeling. We applied our parametric visibility modeling to the gas brightness distribution of the molecular line emission from 12CO J=3-2 and 13CO J=3-2 in the disk around MHO 6, a very-low-mass star in the Taurus star-forming Region. To improve the flux fidelity of our parametric models, we combined models with different pixel resolution before the computation of their visibilities, referred to as ``nesting images.'' When we apply our parametric visibility modeling to MHO 6, with independent fits to the emission from its CO isopotologues, the models return the same consistent results for the stellar mass, disk geometry, and central velocity. The surface height and brightness temperature distribution are also recovered. When compared to other disks, MHO 6 surface height is among the most elevated surfaces, consistent with the predictions for disks around very-low-mass stars. This work demonstrates the feasibility of running rapidly iterable parametric visibility models in moderate resolution and sensitivity interferometric observations. More importantly, this methodology opens the analysis of disk's gas morphology to observations where image-based techniques are unable to robustly operate, as in the case of the compact disk around MHO 6.","Wed, 10 Apr 2024 21:08:34 UTC (1,768 KB)"
"170","Determination of $K^0_S$ Fragmentation Functions including BESIII Measurements and using Neural Networks","Maryam Soleymaninia, Hadi Hashamipour, Maral Salajegheh, Hamzeh Khanpour, Hubert Spiesberger, Ulf-G. Meißner","High Energy Physics - Phenomenology (hep-ph)","In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD. Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data. The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure. To address experimental uncertainties, the Monte Carlo method is employed. Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values. The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties.","Wed, 10 Apr 2024 20:27:33 UTC (10,974 KB)"
"171","An analysis of parameter compression and full-modeling techniques with Velocileptors for DESI 2024 and beyond","M. Maus, S. Chen, M. White, J. Aguilar, S. Ahlen, A. Aviles, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, C. Howlett, M. Ishak, S. Juneau, A. Kremin, Y. Lai, M. Landriau, M. E. Levi, M. Manera, R. Miquel, E. Mueller, A. D. Myers, S. Nadathur, J. Nie, H. E. Noriega, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, S. Ramirez-Solano, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, B. A. Weaver, S. Yuan, P. Zarrouk, H. Zhang, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In anticipation of forthcoming data releases of current and future spectroscopic surveys, we present the validation tests and analysis of systematic effects within \texttt{velocileptors} modeling pipeline when fitting mock data from the \texttt{AbacusSummit} N-body simulations. We compare the constraints obtained from parameter compression methods to the direct fitting (Full-Modeling) approaches of modeling the galaxy power spectra, and show that the ShapeFit extension to the traditional template method is consistent with the Full-Modeling method within the standard $\Lambda$CDM parameter space. We show the dependence on scale cuts when fitting the different redshift bins using the ShapeFit and Full-Modeling methods. We test the ability to jointly fit data from multiple redshift bins as well as joint analysis of the pre-reconstruction power spectrum with the post-reconstruction BAO correlation function signal. We further demonstrate the behavior of the model when opening up the parameter space beyond $\Lambda$CDM and also when combining likelihoods with external datasets, namely the Planck CMB priors. Finally, we describe different parametrization options for the galaxy bias, counterterm, and stochastic parameters, and employ the halo model in order to physically motivate suitable priors that are necessary to ensure the stability of the perturbation theory.","Wed, 10 Apr 2024 19:21:33 UTC (18,470 KB)"
"172","Effective uniaxial dielectric function tensor and optical phonons in ($\bar{2}01$)-plane oriented $β$-Ga$_2$O$_3$ films with equally-distributed six-fold rotation domains","Alyssa Mock, Steffen Richter, Alexis Papamichail, Vallery Stanishev, Misagh Ghezellou, Jawad Ul-Hassan, Andreas Popp, Saud Bin Anooz, Daniella Gogova, Praneeth Ranga, Sriram Krishnamoorthy, Rafal Korlacki, Mathias Schubert, Vanya Darakchieva","Materials Science (cond-mat.mtrl-sci)","Monoclinic $\beta$-Ga$_2$O$_3$ films grown on $c$-plane sapphire have been shown to exhibit six $(\bar{2}01)$-plane oriented domains, which are equally-spaced-by-rotation around the surface normal and equally-sized-by-volume that render the film optical response effectively uniaxial. We derive and discuss an optical model suitable for ellipsometry data analysis of such films. We model mid- and far-infrared ellipsometry data from undoped and electrically insulating films with an effective uniaxial dielectric tensor based on projections of all phonon modes within the rotation domains parallel and perpendicular to the sample normal, i.e., to the reciprocal lattice vector $\mathbf{g}_{\bar{2}01}$. Two effective response functions are described by model, and found sufficient to calculate ellipsometry data that best-match measured ellipsometry data from a representative film. We propose to render either effective dielectric functions, or inverse effective dielectric functions, each separately for electric field directions parallel and perpendicular to $\mathbf{g}_{\bar{2}01}$, by sums of Lorentz oscillators, which permit to determine either sets of transverse optical phonon mode parameters, or sets of longitudinal optical phonon mode parameters, respectively. Transverse optical modes common to both dielectric functions can be traced back to single crystal modes with $B_{\mathrm{u}}$ character, while modes with $A_{\mathrm{u}}$ character only appear within the dielectric function for polarization perpendicular to the sample surface. The thereby obtained parameter sets reveal all phonon modes anticipated from averaging over the six-fold rotation domains of single crystal $\beta$-Ga$_2$O$_3$, but with slightly shifted transverse optical, and completely different longitudinal optical phonon modes.","Wed, 10 Apr 2024 18:49:42 UTC (1,647 KB)"
"173","Quantifying the Errors Introduced by Continuum Scattering Models on the Inferred Structural Properties of Proteins","Rohan S. Adhikari, Dilipkumar N. Asthagiri, Walter G. Chapman","Chemical Physics (physics.chem-ph)","Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs). To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments. For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments. To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data. Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model. However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell. CRYSOL can over-predict Rg by up to 2.5 Angstroms. We rationalize the reason for this behavior and highlight the consequences for force field design.","Wed, 10 Apr 2024 18:36:37 UTC (5,947 KB)"
"174","A comparison between Shapefit compression and Full-Modelling method with PyBird for DESI 2024 and beyond","Yan Lai, Cullan Howlett, Mark Maus, Héctor Gil-Marín, Hernan E. Noriega, Sadi Ramírez-Solano, Pauline Zarrouk, Jessica N. Aguilar, Steven Ahlen, Otávio Alves, Alejandro Aviles, David Brooks, Shi-Fan Chen, Todd Claybaugh, Tamara M. Davis, Kyle Dawson, Axel de la Macorra, Peter Doel, Jaime E. Forero-Romero, Enrique Gaztañaga, Satya Gontcho A Gontcho, Klaus Honscheid, Stephanie Juneau, Martin Landriau, Marc Manera, Ramon Miquel, Eva-Maria Mueller, Seshadri Nadathur, Gustavo Niz, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Mehdi Rezaie, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, David Sprayberry, Gregory Tarlé, Mariana Vargas-Magaña, Licia Verde, Sihan Yuan, Rongpu Zhou, Hu Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","DESI aims to provide one of the tightest constraints on cosmological parameters by analyzing the clustering of more than thirty million galaxies. However, obtaining such constraints requires special care in validating the analysis methods, and efforts to reduce the computational time required through techniques such as data compression and emulation. In this work, we perform a precision validation of the PyBird power spectrum modelling code with both a traditional, but emulated, Full-Modelling approach and the model-independent Shapefit compression approach. Using cubic simulations, which accurately reproduce the clustering and precision of the DESI survey, we find that the cosmological constraints from Shapefit and Full-Modelling are consistent with each other at the $\sim0.3\sigma$ level. Both Shapefit and Full-Modelling are also consistent with the true $\Lambda$CDM simulation cosmology, even when including the hexadecapole, down to a scale $k_{\mathrm{max}} = 0.20 h \mathrm{Mpc}^{-1}$. For extended models such as the $w$CDM and the $o$CDM models, we find including the hexadecapole can significantly improve the constraints and reduce the systematic errors with the same $k_{\mathrm{max}}$. Furthermore, we also show that the constraints on cosmological parameters with the correlation function evaluated from PyBird down to $s_{\mathrm{min}} = 30 h^{-1} \mathrm{Mpc}$ are unbiased, and consistent with the constraints from the power spectrum.","Wed, 10 Apr 2024 18:26:16 UTC (18,726 KB)"
"175","Validating the Galaxy and Quasar Catalog-Level Blinding Scheme for the DESI 2024 analysis","U. Andrade, J. Mena-Fernández, H. Awan, A. J. Ross, S. Brieden, J. Pan, A. de Mattia, J. Aguilar, S. Ahlen, O. Alves, D. Brooks, E. Buckley-Geer, E. Chaussidon, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, J. Guy, C. Hahn, M. M. S Hanif, K. Honscheid, C. Howlett, D. Huterer, S. Juneau, A. Kremin, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. A. Newman, J. Nie, G. Niz, N. Palanque-Delabrouille, W. J. Percival, M. Pinon, C. Poppett, F. Prada, M. Rashkovetskyi, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, L. Verde, B. A. Weaver","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the era of precision cosmology, ensuring the integrity of data analysis through blinding techniques is paramount -- a challenge particularly relevant for the Dark Energy Spectroscopic Instrument (DESI). DESI represents a monumental effort to map the cosmic web, with the goal to measure the redshifts of tens of millions of galaxies and quasars. Given the data volume and the impact of the findings, the potential for confirmation bias poses a significant challenge. To address this, we implement and validate a comprehensive blind analysis strategy for DESI Data Release 1 (DR1), tailored to the specific observables DESI is most sensitive to: Baryonic Acoustic Oscillations (BAO), Redshift-Space Distortion (RSD) and primordial non-Gaussianities (PNG). We carry out the blinding at the catalog level, implementing shifts in the redshifts of the observed galaxies to blind for BAO and RSD signals and weights to blind for PNG through a scale-dependent bias. We validate the blinding technique on mocks, as well as on data by applying a second blinding layer to perform a battery of sanity checks. We find that the blinding strategy alters the data vector in a controlled way such that the BAO and RSD analysis choices do not need any modification before and after unblinding. The successful validation of the blinding strategy paves the way for the unblinded DESI DR1 analysis, alongside future blind analyses with DESI and other surveys.","Wed, 10 Apr 2024 18:24:08 UTC (8,180 KB)"
"176","A comparison of effective field theory models of redshift space galaxy power spectra for DESI 2024 and future surveys","M. Maus, Y. Lai, H. E. Noriega, S. Ramirez-Solano, A. Aviles, S. Chen, S. Fromenteau, H. Gil-Marín, C. Howlett, M. Vargas-Magaña, M. White, P. Zarrouk, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, E. Burtin, T. Claybaugh, S. Cole, K. Dawson, M. Icaza-Lizaola, A. de la Macorra, A. de Mattia, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, M. Ishak, A. Kremin, M. Landriau, L. Le Guillou, M. Manera, R. Miquel, E. Mueller, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, S. Yuan, R. Zhao, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In preparation for the next generation of galaxy redshift surveys, and in particular the year-one data release from the Dark Energy Spectroscopic Instrument (DESI), we investigate the consistency of a variety of effective field theory models that describe the galaxy-galaxy power spectra in redshift space into the quasi-linear regime using 1-loop perturbation theory. These models are employed in the pipelines \texttt{velocileptors}, \texttt{PyBird}, and \texttt{Folps$\nu$}. While these models have been validated independently, a detailed comparison with consistent choices has not been attempted. After briefly discussing the theoretical differences between the models we describe how to provide a more apples-to-apples comparison between them. We present the results of fitting mock spectra from the \texttt{AbacusSummit} suite of N-body simulations provided in three redshift bins to mimic the types of dark time tracers targeted by the DESI survey. We show that the theories behave similarly and give consistent constraints in both the forward-modeling and ShapeFit compressed fitting approaches. We additionally generate (noiseless) synthetic data from each pipeline to be fit by the others, varying the scale cuts in order to show that the models agree within the range of scales for which we expect 1-loop perturbation theory to be applicable. This work lays the foundation of Full-Shape analysis with DESI Y1 galaxy samples where in the tests we performed, we found no systematic error associated with the modeling of the galaxy redshift space power spectrum for this volume.","Wed, 10 Apr 2024 18:03:38 UTC (8,310 KB)"
"177","Comparing Compressed and Full-modeling Analyses with FOLPS: Implications for DESI 2024 and beyond","H. E. Noriega, A. Aviles, H. Gil-Marín, S. Ramirez-Solano, S. Fromenteau, M. Vargas-Magaña, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, J. L. Cervantes-Cota, S. Chen, T. Claybaugh, S. Cole, K. Dawson, A. de la Macorra, A. de Mattia, P. Doel, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, K. Honscheid, J. Hou, C. Howlett, M. Ishak, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, G. Morales-Navarrete, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, L. Verde, S. Yuan, P. Zarrouk, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe. In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos. We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations. We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales. Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy. In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis. Furthermore, we show how opening up the parameter space beyond the vanilla $\Lambda$CDM model affects the DESI observables. These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state. We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest. This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS.","Wed, 10 Apr 2024 18:00:54 UTC (12,699 KB)"
"178","Full Modeling and Parameter Compression Methods in configuration space for DESI 2024 and beyond","S. Ramirez-Solano, M. Icaza-Lizaola, H. E. Noriega, M. Vargas-Magaña, S. Fromenteau, A. Aviles, F. Rodriguez-Martinez, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, B. Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, K. Honscheid, C. Howlett, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. Nie, W. J. Percival, C. Poppett, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, L. Verde, B. A. Weaver, R. H. Wechsler, S. Yuan, P. Zarrouk, H. Zou (DESI Collaboration)","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data. This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space. We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard). We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements. Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory. We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model. Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\sigma$ in all cases for a 1-year DESI volume. Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers. We find good agreement with the results from all these models.","Wed, 10 Apr 2024 18:00:53 UTC (8,730 KB)"
"179","Complete Optimal Non-Resonant Anomaly Detection","Gregor Kasieczka, John Andrew Raine, David Shih, Aman Upadhyay","High Energy Physics - Phenomenology (hep-ph)","We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions. Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature. The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features. Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses. The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\nu \nu) +\text{jets}$, and the data-driven control process is $\gamma+\text{jets}$.","Wed, 10 Apr 2024 18:00:01 UTC (859 KB)"
"180","Language Imbalance Can Boost Cross-lingual Generalisation","Anton Schäfer, Shauli Ravfogel, Thomas Hofmann, Tiago Pimentel, Imanol Schlag","Computation and Language (cs.CL)","Multilinguality is crucial for extending recent advancements in language modelling to diverse linguistic communities. To maintain high performance while representing multiple languages, multilingual models ideally align representations, allowing what is learned in one language to generalise to others. Prior research has emphasised the importance of parallel data and shared vocabulary elements as key factors for such alignment. In this study, we investigate an unintuitive novel driver of cross-lingual generalisation: language imbalance. In controlled experiments on perfectly equivalent cloned languages, we observe that the existence of a predominant language during training boosts the performance of less frequent languages and leads to stronger alignment of model representations across languages. Furthermore, we find that this trend is amplified with scale: with large enough models or long enough training, we observe that bilingual training data with a 90/10 language split yields better performance on both languages than a balanced 50/50 split. Building on these insights, we design training schemes that can improve performance in all cloned languages, even without altering the training data. As we extend our analysis to real languages, we find that infrequent languages still benefit from frequent ones, yet whether language imbalance causes cross-lingual generalisation there is not conclusive.","Thu, 11 Apr 2024 17:58:05 UTC (4,890 KB)"
"181","OSWorld: Benchmarking Multimodal Agents for Open-Ended Tasks in Real Computer Environments","Tianbao Xie, Danyang Zhang, Jixuan Chen, Xiaochuan Li, Siheng Zhao, Ruisheng Cao, Toh Jing Hua, Zhoujun Cheng, Dongchan Shin, Fangyu Lei, Yitao Liu, Yiheng Xu, Shuyan Zhou, Silvio Savarese, Caiming Xiong, Victor Zhong, Tao Yu","Artificial Intelligence (cs.AI)","Autonomous agents that accomplish complex computer tasks with minimal human interventions have the potential to transform human-computer interaction, significantly enhancing accessibility and productivity. However, existing benchmarks either lack an interactive environment or are limited to environments specific to certain applications or domains, failing to reflect the diverse and complex nature of real-world computer use, thereby limiting the scope of tasks and agent scalability. To address this issue, we introduce OSWorld, the first-of-its-kind scalable, real computer environment for multimodal agents, supporting task setup, execution-based evaluation, and interactive learning across various operating systems such as Ubuntu, Windows, and macOS. OSWorld can serve as a unified, integrated computer environment for assessing open-ended computer tasks that involve arbitrary applications. Building upon OSWorld, we create a benchmark of 369 computer tasks involving real web and desktop apps in open domains, OS file I/O, and workflows spanning multiple applications. Each task example is derived from real-world computer use cases and includes a detailed initial state setup configuration and a custom execution-based evaluation script for reliable, reproducible evaluation. Extensive evaluation of state-of-the-art LLM/VLM-based agents on OSWorld reveals significant deficiencies in their ability to serve as computer assistants. While humans can accomplish over 72.36% of the tasks, the best model achieves only 12.24% success, primarily struggling with GUI grounding and operational knowledge. Comprehensive analysis using OSWorld provides valuable insights for developing multimodal generalist agents that were not possible with previous benchmarks. Our code, environment, baseline models, and data are publicly available at this https URL.","Thu, 11 Apr 2024 17:56:05 UTC (40,911 KB)"
"182","An End-to-End Structure with Novel Position Mechanism and Improved EMD for Stock Forecasting","Chufeng Li, Jianyong Chen","Statistical Finance (q-fin.ST)","As a branch of time series forecasting, stock movement forecasting is one of the challenging problems for investors and researchers. Since Transformer was introduced to analyze financial data, many researchers have dedicated themselves to forecasting stock movement using Transformer or attention mechanisms. However, existing research mostly focuses on individual stock information but ignores stock market information and high noise in stock data. In this paper, we propose a novel method using the attention mechanism in which both stock market information and individual stock information are considered. Meanwhile, we propose a novel EMD-based algorithm for reducing short-term noise in stock data. Two randomly selected exchange-traded funds (ETFs) spanning over ten years from US stock markets are used to demonstrate the superior performance of the proposed attention-based method. The experimental analysis demonstrates that the proposed attention-based method significantly outperforms other state-of-the-art baselines. Code is available at this https URL.","Mon, 25 Mar 2024 15:23:22 UTC (1,088 KB)"
"183","Machine Learning-based Approach for Ex-post Assessment of Community Risk and Resilience Based on Coupled Human-infrastructure Systems Performance","Xiangpeng Li, Ali Mostafavi","Computers and Society (cs.CY)","There is a limitation in the literature of data-driven analyses for the ex-post evaluation of community risk and resilience, particularly using features related to the performance of coupled human-infrastructure systems. To address this gap, in this study we created a machine learning-based method for the ex-post assessment of community risk and resilience and their interplay based on features related to the coupled human-infrastructure systems performance. Utilizing feature groups related to population protective actions, infrastructure/building performance features, and recovery features, we examined the risk and resilience performance of communities in the context of the 2017 Hurricane Harvey in Harris County, Texas. These features related to the coupled human-infrastructure systems performance were processed using the K-means clustering method to classify census block groups into four distinct clusters then, based on feature analysis, these clusters were labeled and designated into four quadrants of risk-resilience archetypes. Finally, we analyzed the disparities in risk-resilience status of spatial areas across different clusters as well as different income groups. The findings unveil the risk-resilience status of spatial areas shaped by their coupled human-infrastructure systems performance and their interactions. The results also inform about features that contribute to high resilience in high-risk areas. For example, the results indicate that in high-risk areas, evacuation rates contributed to a greater resilience, while in low-risk areas, preparedness contributed to greater resilience.","Sun, 24 Mar 2024 19:32:23 UTC (2,889 KB)"
"184","Visualization for physics analysis improvement and applications in BESIII","Zhi-Jun Li, Ming-Kuan Yuan, Yun-Xuan Song, Yan-Gu Li, Jing-Shu Li, Sheng-Sen Sun, Xiao-Long Wang, Zheng-Yun You, Ya-Jun Mao","Data Analysis, Statistics and Probability (physics.data-an)","Modern particle physics experiments usually rely on highly complex and large-scale spectrometer devices. In high energy physics experiments, visualization helps detector design, data quality monitoring, offline data processing, and has great potential for improving physics analysis. In addition to the traditional physics data analysis based on statistical methods, visualization provides unique intuitive advantages in searching for rare signal events and reducing background noises. By applying the event display tool to several physics analyses in the BESIII experiment, we demonstrate that visualization can benefit potential physics discovery and improve the signal significance. With the development of modern visualization techniques, it is expected to play a more important role in future data processing and physics analysis of particle physics experiments.","Tue, 19 Mar 2024 10:26:48 UTC (1,990 KB)"
"185","Distributed Record Linkage in Healthcare Data with Apache Spark","Mohammad Heydari, Reza Sarshar, Mohammad Ali Soltanshahi","Distributed, Parallel, and Cluster Computing (cs.DC)","Healthcare data is a valuable resource for research, analysis, and decision-making in the medical field. However, healthcare data is often fragmented and distributed across various sources, making it challenging to combine and analyze effectively. Record linkage, also known as data matching, is a crucial step in integrating and cleaning healthcare data to ensure data quality and accuracy. Apache Spark, a powerful open-source distributed big data processing framework, provides a robust platform for performing record linkage tasks with the aid of its machine learning library. In this study, we developed a new distributed data-matching model based on the Apache Spark Machine Learning library. To ensure the correct functioning of our model, the validation phase has been performed on the training data. The main challenge is data imbalance because a large amount of data is labeled false, and a small number of records are labeled true. By utilizing SVM and Regression algorithms, our results demonstrate that research data was neither over-fitted nor under-fitted, and this shows that our distributed model works well on the data.","Sat, 9 Mar 2024 05:18:15 UTC (249 KB)"
"186","Rate-Optimal Non-Asymptotics for the Quadratic Prediction Error Method","Charis Stamouli, Ingvar Ziemann, George J. Pappas","Statistics Theory (math.ST)","We study the quadratic prediction error method -- i.e., nonlinear least squares -- for a class of time-varying parametric predictor models satisfying a certain identifiability condition. While this method is known to asymptotically achieve the optimal rate for a wide range of problems, there have been no non-asymptotic results matching these optimal rates outside of a select few, typically linear, model classes. By leveraging modern tools from learning with dependent data, we provide the first rate-optimal non-asymptotic analysis of this method for our more general setting of nonlinearly parametrized model classes. Moreover, we show that our results can be applied to a particular class of identifiable AutoRegressive Moving Average (ARMA) models, resulting in the first optimal non-asymptotic rates for identification of ARMA models.","Thu, 11 Apr 2024 17:36:28 UTC (45 KB)"
"187","Astrophysics and Nuclear Physics Informed Interactions in Dense Matter: Insights from PSR J0437-4715","Tuhin Malik, V. Dexheimer, Constança Providência","Nuclear Theory (nucl-th)","We investigate how vector and isovector interactions can be determined within the density regime of neutron stars, while fulfilling nuclear and astrophysics constrains. We make use of the Chiral Mean Field (CMF) model, a SU(3) nonlinear realization of the sigma model within the mean-field approximation, for the first time within a Bayesian analysis framework. We show that neutron-matter $\chi$EFT constraints at low density are only satisfied if the vector-isovector mixed interaction term is included, e.g., a $\omega^2\rho^2$ term. We also show the behavior of the model with respect to the conformal limit. We demonstrate that the CMF model is able to predict a value for the parameter $d_c$ related to the trace anomaly and its derivative takes values below 0.2 above four times saturation density within a hadronic model that does not include a phase transition to deconfined matter. We compare these effects with results from other (non-chiral) Relativistic Mean Field models to assess how different approaches to incorporating the same physical constraints affect predictions of neutron-star properties and dense matter equations of state. We also include data from the gravitation wave event GW230529 detected by the LIGO-Virgo-Kagra collaboration and the most recent radius measurement of PSR J0437-4715 from the NASA NICER mission. Our analysis reveals that this new NICER measurement leads to an average reduction of approximately $\sim 0.15$ km radius in the posterior of the neutron-star mass-radius relationship.","Thu, 11 Apr 2024 17:35:54 UTC (1,088 KB)"
"188","Robust elastic full-waveform inversion using an alternating direction method of multipliers with reconstructed wavefields","Kamal Aghazade, Ali Gholami, Hossein S. Aghamiry, Hamid Reza Siahkoohi","Numerical Analysis (math.NA)","Elastic full-waveform inversion (EFWI) is a process used to estimate subsurface properties by fitting seismic data while satisfying wave propagation physics. The problem is formulated as a least-squares data fitting minimization problem with two sets of constraints: Partial-differential equation (PDE) constraints governing elastic wave propagation and physical model constraints implementing prior information. The alternating direction method of multipliers is used to solve the problem, resulting in an iterative algorithm with well-conditioned subproblems. Although wavefield reconstruction is the most challenging part of the iteration, sparse linear algebra techniques can be used for moderate-sized problems and frequency domain formulations. The Hessian matrix is blocky with diagonal blocks, making model updates fast. Gradient ascent is used to update Lagrange multipliers by summing PDE violations. Various numerical examples are used to investigate algorithmic components, including model parameterizations, physical model constraints, the role of the Hessian matrix in suppressing interparameter cross-talk, computational efficiency with the source sketching method, and the effect of noise and near-surface effects.","Thu, 11 Apr 2024 17:25:23 UTC (16,221 KB)"
"189","Snake Story: Exploring Game Mechanics for Mixed-Initiative Co-creative Storytelling Games","Daijin Yang, Erica Kleinman, Giovanni Maria Troiano, Elina Tochilnikova, Casper Harteveld","Human-Computer Interaction (cs.HC)","Mixed-initiative co-creative storytelling games have existed for some time as a way to merge storytelling with play. However, modern mixed-initiative co-creative storytelling games predominantly prioritize story creation over gameplay mechanics, which might not resonate with all players. As such, there is untapped potential for creating mixed-initiative games with more complex mechanics in which players can engage with both co-creation and gameplay goals. To explore the potential of more prominent gameplay in mixed-initiative co-creative storytelling games, we created Snake Story, a variation of the classic Snake game featuring a human-AI co-writing element. To explore how players interact with the mixed-initiative game, we conducted a qualitative playtest with 11 participants. Analysis of both think-aloud and interview data revealed that players' strategies and experiences were affected by their perception of Snake Story as either a collaborative tool, a traditional game, or a combination of both. Based on these findings, we present design considerations for future development in mixed-initiative co-creative gaming.","Thu, 11 Apr 2024 16:40:24 UTC (1,038 KB)"
"190","Revisiting a drag partition model for canopy-like roughness elements","Elia Buono, Gabriel G. Katul, Davide Vettori, Davide Poggi, Costantino Manes","Fluid Dynamics (physics.flu-dyn)","Turbulent flows over a large surface area (S) covered by n obstacles experience an overall drag due to the presence of the ground and the protruding obstacles into the flow. The drag partition between the roughness obstacles and the ground is analyzed using an analytical model proposed by Raupach (1992) and is hereafter referred to as R92. The R92 is based on the premise that the wake behind an isolated roughness element can be described by a shelter area A and a shelter volume V. The individual sizes of A and V without any interference from other obstacles can be determined from scaling analysis for the spread of wakes. To upscale from an individual roughness element to n/S elements where wakes may interact, R92 adopted a background stress re-normalizing instead of reducing A or V with each element addition. This work shows that R92's approach only converges to a linear reduction in A and V for small n/S where wakes have low probability of interacting with one another. This probabilistic nature suggests that up-scaling from individual to multiple roughness elements can be re-formulated using stochastic averaging methods proposed here. The two approaches are shown to recover R92 under plausible conditions. Comparisons between R92 and available data on blocks and vegetation-like roughness elements confirm the practical utility of R92 and its potential use in large-scale models provided the relevant parameters accommodate certain features of the roughness element type (cube versus vegetation-like) and, to a lesser extent, their configuration throughout S.","Thu, 11 Apr 2024 16:27:43 UTC (359 KB)"
"191","Diagram Analysis of Iterative Algorithms","Chris Jones, Lucas Pesenti","Computational Complexity (cs.CC)","We study a general class of first-order iterative algorithms which includes power iteration, belief propagation and Approximate Message Passing (AMP), and many forms of gradient descent. When the input is a random matrix with i.i.d. entries, we present a new way to analyze these algorithms using combinatorial diagrams. Each diagram is a small graph, and the operations of the algorithm correspond to simple combinatorial operations on these graphs.
We prove a fundamental property of the diagrams: asymptotically, we can discard all of the diagrams except for the trees. The mechanics of first-order algorithms simplify dramatically as the algorithmic operations have particularly simple and interpretable effects on the trees. We further show that the tree-shaped diagrams are essentially a basis of asymptotically independent Gaussian vectors.
The tree approximation mirrors the assumption of the cavity method, a 40-year-old non-rigorous technique in statistical physics which has served as one of the most fundamental techniques in the field. We demonstrate the connection with the replica symmetric cavity method by ""implementing"" heuristic physics derivations into rigorous proofs. We rigorously establish that belief propagation is asymptotically equal to its associated AMP algorithm and we give a new simple proof of the state evolution formula for AMP.
These results apply when the iterative algorithm runs for constantly many iterations. We then push the diagram analysis to a number of iterations that scales with the dimension $n$ of the input matrix. We prove that for debiased power iteration, the tree diagram representation accurately describes the dynamic all the way up to $n^{\Omega(1)}$ iterations. We conjecture that this can be extended up to $n^{1/2}$ iterations but no further. Our proofs use straightforward combinatorial arguments akin to the trace method from random matrix theory.","Thu, 11 Apr 2024 16:10:52 UTC (1,097 KB)"
"192","The Power of Properties: Uncovering the Influential Factors in Emotion Classification","Tim Büchner, Niklas Penzel, Orlando Guntinas-Lichius, Joachim Denzler","Computer Vision and Pattern Recognition (cs.CV)","Facial expression-based human emotion recognition is a critical research area in psychology and medicine. State-of-the-art classification performance is only reached by end-to-end trained neural networks. Nevertheless, such black-box models lack transparency in their decision-making processes, prompting efforts to ascertain the rules that underlie classifiers' decisions. Analyzing single inputs alone fails to expose systematic learned biases. These biases can be characterized as facial properties summarizing abstract information like age or medical conditions. Therefore, understanding a model's prediction behavior requires an analysis rooted in causality along such selected properties. We demonstrate that up to 91.25% of classifier output behavior changes are statistically significant concerning basic properties. Among those are age, gender, and facial symmetry. Furthermore, the medical usage of surface electromyography significantly influences emotion prediction. We introduce a workflow to evaluate explicit properties and their impact. These insights might help medical professionals select and apply classifiers regarding their specialized data and properties.","Thu, 11 Apr 2024 16:01:00 UTC (895 KB)"
"193","Global solution and singularity formation for the supersonic expanding wave of compressible Euler equations with radial symmetry","Geng Chen, Faris A. El-Katri, Yanbo Hu, Yannan Shen","Analysis of PDEs (math.AP)","In this paper, we define the rarefaction and compression characters for the supersonic expanding wave of the compressible Euler equations with radial symmetry. Under this new definition, we show that solutions with rarefaction initial data will not form shock in finite time, i.e. exist global-in-time as classical solutions. On the other hand, singularity forms in finite time when the initial data include strong compression somewhere. Several useful invariant domains will be also given.","Thu, 11 Apr 2024 15:15:35 UTC (34 KB)"
"194","iPREFER: An Intelligent Parameter Extractor based on Features for BSIM-CMG Models","Zhiliang Peng, Yicheng Wang, Zhengwu Yuan, Xingsheng Wang","Systems and Control (eess.SY)","This paper introduces an innovative parameter extraction method for BSIM-CMG compact models, seamlessly integrating curve feature extraction and machine learning techniques. This method offers a promising solution for bridging the division between TCAD and compact model, significantly contributing to the Design Technology Co-Optimization (DTCO) process. The key innovation lies in the development of an automated IV and CV curve feature extractor, which not only streamlines the analysis of device IV and CV curves but also enhances the consistency and efficiency of data processing. Validation on 5-nm nanosheet devices underscores the extractor's remarkable precision, with impressively low fitting errors of 0.42% for CV curves and 1.28% for IV curves. Furthermore, its adaptability to parameter variations, including those in Equivalent Oxide Thickness and Gate Length, solidifies its potential to revolutionize the TCAD-to-compact model transition. This universal BSIM-CMG model parameter extractor promises to improve the DTCO process, offering efficient process optimization and accurate simulations for semiconductor device performance prediction.","Thu, 11 Apr 2024 15:10:10 UTC (898 KB)"
"195","Trials Factor for Semi-Supervised NN Classifiers in Searches for Narrow Resonances at the LHC","Benjamin Lieberman, Andreas Crivellin, Salah-Eddine Dahbi, Finn Stevenson, Nidhi Tripathi, Mukesh Kumar, Bruce Mellado","High Energy Physics - Phenomenology (hep-ph)","To mitigate the model dependencies of searches for new narrow resonances at the Large Hadron Collider (LHC), semi-supervised Neural Networks (NNs) can be used. Unlike fully supervised classifiers these models introduce an additional look-elsewhere effect in the process of optimising thresholds on the response distribution. We perform a frequentist study to quantify this effect, in the form of a trials factor. As an example, we consider simulated $Z\gamma$ data to perform narrow resonance searches using semi-supervised NN classifiers. The results from this analysis provide substantiation that the look-elsewhere effect induced by the semi-supervised NN is under control.","Thu, 11 Apr 2024 15:06:15 UTC (2,225 KB)"
"196","The Cattaneo-Christov approximation of Fourier heat-conductive compressible fluids","Timothée Crin-Barat, Shuichi Kawashima, Jiang Xu","Analysis of PDEs (math.AP)","We investigate the Navier-Stokes-Cattaneo-Christov (NSC) system in $\mathbb{R}^d$ ($d\geq3$), a model of heat-conductive compressible flows serving as a finite speed of propagation approximation of the Navier-Stokes-Fourier (NSF) system. Due to the presence of Oldroyd's upper-convected derivatives, the system (NSC) exhibits a \textit{lack of hyperbolicity} which makes it challenging to establish its well-posedness, especially in multi-dimensional contexts. In this paper, within a critical regularity functional framework, we prove the global-in-time well-posedness of (NSC) for initial data that are small perturbations of constant equilibria, uniformly with respect to the approximation parameter $\varepsilon>0$. Then, building upon this result, we obtain the sharp large-time asymptotic behaviour of (NSC) and, for all time $t>0$, we derive quantitative error estimates between the solutions of (NSC) and (NSF). To the best of our knowledge, our work provides the first strong convergence result for this relaxation procedure in the three-dimensional setting and for ill-prepared data.
The (NSC) system is partially dissipative and incorporates both partial diffusion and partial damping mechanisms. To address these aspects and ensure the large-time stability of the solutions, we construct localized-in-frequency perturbed energy functionals based on the hypocoercivity theory. More precisely, our analysis relies on partitioning the frequency space into \textit{three} distinct regimes: low, medium and high frequencies. Within each frequency regime, we introduce effective unknowns and Lyapunov functionals, revealing the spectrally expected dissipative structures.","Thu, 11 Apr 2024 14:52:25 UTC (70 KB)"
"197","Generating Synthetic Satellite Imagery With Deep-Learning Text-to-Image Models -- Technical Challenges and Implications for Monitoring and Verification","Tuong Vy Nguyen, Alexander Glaser, Felix Biessmann","Computer Vision and Pattern Recognition (cs.CV)","Novel deep-learning (DL) architectures have reached a level where they can generate digital media, including photorealistic images, that are difficult to distinguish from real data. These technologies have already been used to generate training data for Machine Learning (ML) models, and large text-to-image models like DALL-E 2, Imagen, and Stable Diffusion are achieving remarkable results in realistic high-resolution image generation. Given these developments, issues of data authentication in monitoring and verification deserve a careful and systematic analysis: How realistic are synthetic images? How easily can they be generated? How useful are they for ML researchers, and what is their potential for Open Science? In this work, we use novel DL models to explore how synthetic satellite images can be created using conditioning mechanisms. We investigate the challenges of synthetic satellite image generation and evaluate the results based on authenticity and state-of-the-art metrics. Furthermore, we investigate how synthetic data can alleviate the lack of data in the context of ML methods for remote-sensing. Finally we discuss implications of synthetic satellite imagery in the context of monitoring and verification.","Thu, 11 Apr 2024 14:00:20 UTC (23,169 KB)"
"198","3D-CSAD: Untrained 3D Anomaly Detection for Complex Manufacturing Surfaces","Xuanming Cao, Chengyu Tao, Juan Du","Computer Vision and Pattern Recognition (cs.CV)","The surface quality inspection of manufacturing parts based on 3D point cloud data has attracted increasing attention in recent years. The reason is that the 3D point cloud can capture the entire surface of manufacturing parts, unlike the previous practices that focus on some key product characteristics. However, achieving accurate 3D anomaly detection is challenging, due to the complex surfaces of manufacturing parts and the difficulty of collecting sufficient anomaly samples. To address these challenges, we propose a novel untrained anomaly detection method based on 3D point cloud data for complex manufacturing parts, which can achieve accurate anomaly detection in a single sample without training data. In the proposed framework, we transform an input sample into two sets of profiles along different directions. Based on one set of the profiles, a novel segmentation module is devised to segment the complex surface into multiple basic and simple components. In each component, another set of profiles, which have the nature of similar shapes, can be modeled as a low-rank matrix. Thus, accurate 3D anomaly detection can be achieved by using Robust Principal Component Analysis (RPCA) on these low-rank matrices. Extensive numerical experiments on different types of parts show that our method achieves promising results compared with the benchmark methods.","Thu, 11 Apr 2024 13:46:05 UTC (6,818 KB)"
"199","Depth Estimation using Weighted-loss and Transfer Learning","Muhammad Adeel Hafeez, Michael G. Madden, Ganesh Sistu, Ihsan Ullah","Computer Vision and Pattern Recognition (cs.CV)","Depth estimation from 2D images is a common computer vision task that has applications in many fields including autonomous vehicles, scene understanding and robotics. The accuracy of a supervised depth estimation method mainly relies on the chosen loss function, the model architecture, quality of data and performance metrics. In this study, we propose a simplified and adaptable approach to improve depth estimation accuracy using transfer learning and an optimized loss function. The optimized loss function is a combination of weighted losses to which enhance robustness and generalization: Mean Absolute Error (MAE), Edge Loss and Structural Similarity Index (SSIM). We use a grid search and a random search method to find optimized weights for the losses, which leads to an improved model. We explore multiple encoder-decoder-based models including DenseNet121, DenseNet169, DenseNet201, and EfficientNet for the supervised depth estimation model on NYU Depth Dataset v2. We observe that the EfficientNet model, pre-trained on ImageNet for classification when used as an encoder, with a simple upsampling decoder, gives the best results in terms of RSME, REL and log10: 0.386, 0.113 and 0.049, respectively. We also perform a qualitative analysis which illustrates that our model produces depth maps that closely resemble ground truth, even in cases where the ground truth is flawed. The results indicate significant improvements in accuracy and robustness, with EfficientNet being the most successful architecture.","Thu, 11 Apr 2024 12:25:54 UTC (2,233 KB)"
"200","Run-time Monitoring of 3D Object Detection in Automated Driving Systems Using Early Layer Neural Activation Patterns","Hakan Yekta Yatbaz, Mehrdad Dianati, Konstantinos Koufos, Roger Woodman","Computer Vision and Pattern Recognition (cs.CV)","Monitoring the integrity of object detection for errors within the perception module of automated driving systems (ADS) is paramount for ensuring safety. Despite recent advancements in deep neural network (DNN)-based object detectors, their susceptibility to detection errors, particularly in the less-explored realm of 3D object detection, remains a significant concern. State-of-the-art integrity monitoring (also known as introspection) mechanisms in 2D object detection mainly utilise the activation patterns in the final layer of the DNN-based detector's backbone. However, that may not sufficiently address the complexities and sparsity of data in 3D object detection. To this end, we conduct, in this article, an extensive investigation into the effects of activation patterns extracted from various layers of the backbone network for introspecting the operation of 3D object detectors. Through a comparative analysis using Kitti and NuScenes datasets with PointPillars and CenterPoint detectors, we demonstrate that using earlier layers' activation patterns enhances the error detection performance of the integrity monitoring system, yet increases computational complexity. To address the real-time operation requirements in ADS, we also introduce a novel introspection method that combines activation patterns from multiple layers of the detector's backbone and report its performance.","Thu, 11 Apr 2024 12:24:47 UTC (10,043 KB)"
"201","Merger Analysis with Latent Price","Paul Koh","Econometrics (econ.EM)","Standard empirical tools for merger analysis assume price data, which may not be readily available. This paper characterizes sufficient conditions for identifying the unilateral effects of mergers without price data. I show that revenues, margins, and revenue diversion ratios are sufficient for identifying the gross upward pricing pressure indices, impact on consumer/producer surplus, and compensating marginal cost reductions associated with a merger. I also describe assumptions on demand that facilitate the identification of revenue diversion ratios and merger simulations. I use the proposed framework to evaluate the Staples/Office Depot merger (2016).","Thu, 11 Apr 2024 12:23:57 UTC (43 KB)"
"202","Consistent Distribution Free Affine Invariant Tests for the Validity of Independent Component Models","Marc Hallin, Simos G. Meintanis, Klaus Nordhausen","Methodology (stat.ME)","We propose a family of tests of the validity of the assumptions underlying independent component analysis methods. The tests are formulated as L2-type procedures based on characteristic functions and involve weights; a proper choice of these weights and the estimation method for the mixing matrix yields consistent and affine-invariant tests. Due to the complexity of the asymptotic null distribution of the resulting test statistics, implementation is based on permutational and resampling strategies. This leads to distribution-free procedures regardless of whether these procedures are performed on the estimated independent components themselves or the componentwise ranks of their components. A Monte Carlo study involving various estimation methods for the mixing matrix, various weights, and a competing test based on distance covariance is conducted under the null hypothesis as well as under alternatives. A real-data application demonstrates the practical utility and effectiveness of the method.","Thu, 11 Apr 2024 10:48:23 UTC (593 KB)"
"203","Lower semicontinuity and existence results for anisotropic TV functionals with signed measure data","Eleonora Ficola, Thomas Schmidt","Analysis of PDEs (math.AP)","We study the minimization of anisotropic total variation functionals with additional measure terms among functions of bounded variation subject to a Dirichlet boundary condition. More specifically, we identify and characterize certain isoperimetric conditions, which prove to be sharp assumptions on the signed measure data in connection with semicontinuity, existence, and relaxation results. Furthermore, we present a variety of examples which elucidate our assumptions and results.","Thu, 11 Apr 2024 10:44:07 UTC (270 KB)"
"204","State-Space Modeling of Shape-constrained Functional Time Series","Daichi Hiraki, Yasuyuki Hamura, Kaoru Irie, Shonosuke Sugasawa","Applications (stat.AP)","Functional time series data frequently appears in economic applications, where the functions of interest are subject to some shape constraints, including monotonicity and convexity, as typical of the estimation of the Lorenz curve. This paper proposes a state-space model for time-varying functions to extract trends and serial dependence from functional time series while imposing the shape constraints on the estimated functions. The function of interest is modeled by a convex combination of selected basis functions to satisfy the shape constraints, where the time-varying convex weights on simplex follow the dynamic multi-logit models. For the complicated likelihood of this model, a novel data augmentation technique is devised to enable posterior computation by an efficient Markov chain Monte Carlo method. The proposed method is applied to the estimation of time-varying Lorenz curves, and its utility is illustrated through numerical experiments and analysis of panel data of household incomes in Japan.","Thu, 11 Apr 2024 09:17:30 UTC (1,484 KB)"
"205","M-scan: A Multi-Scenario Causal-driven Adaptive Network for Recommendation","Jiachen Zhu, Yichao Wang, Jianghao Lin, Jiarui Qin, Ruiming Tang, Weinan Zhang, Yong Yu","Information Retrieval (cs.IR)","We primarily focus on the field of multi-scenario recommendation, which poses a significant challenge in effectively leveraging data from different scenarios to enhance predictions in scenarios with limited data. Current mainstream efforts mainly center around innovative model network architectures, with the aim of enabling the network to implicitly acquire knowledge from diverse scenarios. However, the uncertainty of implicit learning in networks arises from the absence of explicit modeling, leading to not only difficulty in training but also incomplete user representation and suboptimal performance. Furthermore, through causal graph analysis, we have discovered that the scenario itself directly influences click behavior, yet existing approaches directly incorporate data from other scenarios during the training of the current scenario, leading to prediction biases when they directly utilize click behaviors from other scenarios to train models. To address these problems, we propose the Multi-Scenario Causal-driven Adaptive Network M-scan). This model incorporates a Scenario-Aware Co-Attention mechanism that explicitly extracts user interests from other scenarios that align with the current scenario. Additionally, it employs a Scenario Bias Eliminator module utilizing causal counterfactual inference to mitigate biases introduced by data from other scenarios. Extensive experiments on two public datasets demonstrate the efficacy of our M-scan compared to the existing baseline models.","Thu, 11 Apr 2024 09:13:52 UTC (2,290 KB)"
"206","GAN-based iterative motion estimation in HASTE MRI","Mathias S. Feinler, Bernadette N. Hahn","Numerical Analysis (math.NA)","Magnetic Resonance Imaging allows high resolution data acquisition with the downside of motion sensitivity due to relatively long acquisition times. Even during the acquisition of a single 2D slice, motion can severely corrupt the image. Retrospective motion correction strategies do not interfere during acquisition time but operate on the motion affected data. Known methods suited to this scenario are compressed sensing (CS), generative adversarial networks (GANs), and explicit motion estimation. In this paper we propose an iterative approach which uses GAN predictions for motion estimation. The motion estimates allow to provide data consistent reconstructions and can improve reconstruction quality and reliability. With this approach, a clinical application of motion estimation is feasible without any further requirements on the acquisition trajectory i.e. no temporal redundancy is needed. We evaluate our proposed supervised network on motion corrupted HASTE acquisitions of brain and abdomen.","Thu, 11 Apr 2024 09:07:57 UTC (6,372 KB)"
"207","Bayesian Inference with Gaussian Processes for the Determination of Parton Distribution Functions","Alessandro Candido, Luigi Del Debbio, Tommaso Giani, Giacomo Petrillo","High Energy Physics - Phenomenology (hep-ph)","We discuss a Bayesian methodology for the solution of the inverse problem underlying the determination of parton distribution functions (PDFs). In our approach, Gaussian Processes (GPs) are used to model the PDF prior, while Bayes theorem is used in order to determine the posterior distribution of the PDFs given a set of data. We discuss the general formalism, the Bayesian inference at the level of both parameters and hyperparameters, and the simplifications which occur when the observable entering the analysis is linear in the PDF. We benchmark the new methodology in two simple examples for the determination of a single PDF flavor from a set of Deep Inelastic Scattering (DIS) data and from a set of equal-time correlators computed using lattice QCD. We discuss our results, showing how the proposed methodology allows for a well-defined statistical interpretation of the different sources of errors entering the PDF uncertainty, and how results can be validated a posteriori.","Thu, 11 Apr 2024 09:03:52 UTC (442 KB)"
"208","Orbital dynamics in the GG Tau A system: investigating its enigmatic disc","Claudia Toci, Simone Ceppi, Nicolás Cuello, Gaspard Duchêne, Enrico Ragusa, Giuseppe Lodato, Francesca Farina, François Ménard, Hossam Aly","Earth and Planetary Astrophysics (astro-ph.EP)","GG Tau is one of the most studied multiple young stellar systems: GG Tau A is a hierarchical triple surrounded by a massive disc and its companion, GG Tau B, is also a binary. Despite numerous observational attempts, an understanding of the geometry of the GG Tau A system is still elusive. We provide new astrometric measures of the system and we run a set of hydrodynamical simulations with two representative orbits to test how they impact a disc composed of dust and gas. We test the dynamical evolution of the two scenarios on short and long timescales. We obtain synthetic flux emission from our simulations and we compare them with 1300 $\mu$m ALMA dust continuum emission and 1.67 $\mu$m SPHERE dust scattering images to infer the most likely orbital arrangement. We extend the analysis of the binary orbital parameters using six new epochs from archival data, showing that the current measurements alone are not capable of breaking the degeneracy between families of coplanar and misaligned orbits. We found that the time-scale for the onset of the disc eccentricity growth, $\tau_{ecc}$, is a fundamental time-scale for the morphology of the system. Results from numerical simulations show that the best match between is obtained with the misaligned configuration ($\Delta\theta= 30^\circ$) on timescales shorter than $\tau_{ecc}$. The results exhibit an almost circular cavity and dust ring. However, for both scenarios, the cavity size and its eccentricity quickly grow for timescales longer than $\tau_{ecc}$ and the models do not reproduce the observed morphology anymore. This implies that either the age of the system is shorter than $\tau_{ecc}$ or that the disc eccentricity growth is not triggered or dissipated. This finding raises questions on the future evolution of the GG Tau A system and, more in general, on the time evolution of eccentric binaries and their circumbinary discs.","Thu, 11 Apr 2024 08:51:50 UTC (9,675 KB)"
"209","X-ray polarimetric features of Gamma-ray Bursts across varied redshifts and hints for Axion-Like-Particles","Qingxiang Zhang, Feng Huang, Zhongxiang Wang, Taotao Fang","High Energy Astrophysical Phenomena (astro-ph.HE)","Polarimetric features during the prompt phase of Gamma-ray Bursts (GRBs) have been essential for elucidating the debated emission mechanisms and gaining insight into the inner structure of GRBs. However, the potential impact of photon-Axion-Like-Particle (ALP) mixing in extragalactic magnetic fields, leading to significant modifications to the initial polarization state, has been overlooked in discussions concerning prompt phase constraints. In this work, we first examine the statistical characteristics of linear polarization degree ($\Pi_{L}$) in GRBs, by utilizing data from polarimetric missions focusing on sub-MeV emissions. Our analysis, conducted with a restricted sample of GRBs spanning various redshifts, reveals a diverse distribution of $\Pi_{L}$, which currently shows no correlation with the GRBs' spectral parameters or properties of candidate host galaxies. We then explore alternations to the initial $\Pi_{L}$ due to photon-ALP mixing within a domain-like structure of the intergalactic magnetic field (${\bf B}_{\rm IGM} $). With the existence of ALPs with $m_{a}$$~$$\lesssim$$~$$10^{-14}$$~$eV and $g_{a\gamma}~$$\simeq$$~0.5\times10^{-11}$, the mixing leads to a decrease in the polarization degree of initially fully linearly polarized photons, while it induces a certain degree of polarization to initially unpolarized photons. To ensure that the effect of mixing is small enough to be negligible, the mixing term $\Delta_{a\gamma} \equiv 1/2\ g_{a\gamma} {\bf B}_{\rm IGM}$ should be less than $1.5\times 10^{-4}$ Mpc$^{-1}$. Currently, the number of GRBs with both sub-MeV polarization measurement and redshift confirmation remains very limited. Certification of redshift for GRBs with low $\Pi_{L}$ would further constrain the parameter space of ALPs or provide an independent means to determine the upper limit on ${\bf B}_{\rm IGM}$.","Thu, 11 Apr 2024 08:36:21 UTC (2,354 KB)"
"210","How is Visual Attention Influenced by Text Guidance? Database and Model","Yinan Sun, Xiongkuo Min, Huiyu Duan, Guangtao Zhai","Computer Vision and Pattern Recognition (cs.CV)","The analysis and prediction of visual attention have long been crucial tasks in the fields of computer vision and image processing. In practical applications, images are generally accompanied by various text descriptions, however, few studies have explored the influence of text descriptions on visual attention, let alone developed visual saliency prediction models considering text guidance. In this paper, we conduct a comprehensive study on text-guided image saliency (TIS) from both subjective and objective perspectives. Specifically, we construct a TIS database named SJTU-TIS, which includes 1200 text-image pairs and the corresponding collected eye-tracking data. Based on the established SJTU-TIS database, we analyze the influence of various text descriptions on visual attention. Then, to facilitate the development of saliency prediction models considering text influence, we construct a benchmark for the established SJTU-TIS database using state-of-the-art saliency models. Finally, considering the effect of text descriptions on visual attention, while most existing saliency models ignore this impact, we further propose a text-guided saliency (TGSal) prediction model, which extracts and integrates both image features and text features to predict the image saliency under various text-description conditions. Our proposed model significantly outperforms the state-of-the-art saliency models on both the SJTU-TIS database and the pure image saliency databases in terms of various evaluation metrics. The SJTU-TIS database and the code of the proposed TGSal model will be released at: this https URL.","Thu, 11 Apr 2024 08:03:23 UTC (3,122 KB)"
"211","On the convergence analysis of one-shot inversion methods","Marcella Bonazzoli (IDEFIX), Houssem Haddar (IDEFIX), Tuan Anh Vu (IDEFIX)","Numerical Analysis (math.NA)","When an inverse problem is solved by a gradient-based optimization algorithm, the corresponding forward and adjoint problems, which are introduced to compute the gradient, can be also solved iteratively. The idea of iterating at the same time on the inverse problem unknown and on the forward and adjoint problem solutions yields the concept of one-shot inversion methods. We are especially interested in the case where the inner iterations for the direct and adjoint problems are incomplete, that is, stopped before achieving a high accuracy on their solutions. Here, we focus on general linear inverse problems and generic fixed-point iterations for the associated forward problem. We analyze variants of the so-called multi-step one-shot methods, in particular semi-implicit schemes with a regularization parameter. We establish sufficient conditions on the descent step for convergence, by studying the eigenvalues of the block matrix of the coupled iterations. Several numerical experiments are provided to illustrate the convergence of these methods in comparison with the classical gradient descent, where the forward and adjoint problems are solved exactly by a direct solver instead. We observe that very few inner iterations are enough to guarantee good convergence of the inversion algorithm, even in the presence of noisy data.","Thu, 11 Apr 2024 07:40:41 UTC (3,679 KB)"
"212","Characterizing the Influence of Topology on Graph Learning Tasks","Kailong Wu, Yule Xie, Jiaxin Ding, Yuxiang Ren, Luoyi Fu, Xinbing Wang, Chenghu Zhou","Machine Learning (cs.LG)","Graph neural networks (GNN) have achieved remarkable success in a wide range of tasks by encoding features combined with topology to create effective representations. However, the fundamental problem of understanding and analyzing how graph topology influences the performance of learning models on downstream tasks has not yet been well understood. In this paper, we propose a metric, TopoInf, which characterizes the influence of graph topology by measuring the level of compatibility between the topological information of graph data and downstream task objectives. We provide analysis based on the decoupled GNNs on the contextual stochastic block model to demonstrate the effectiveness of the metric. Through extensive experiments, we demonstrate that TopoInf is an effective metric for measuring topological influence on corresponding tasks and can be further leveraged to enhance graph learning.","Thu, 11 Apr 2024 06:04:06 UTC (606 KB)"
"213","Model-independent way to determine the Hubble constant and the curvature from phase shift of gravitational waves with DECIGO","Tonghua Liu, Shuo Cao, Marek Biesiada, Yilong Zhang, Jieci Wang","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In this Letter, we propose a model-independent method to determine the Hubble constant and curvature simultaneously taking advantage of the possibilities of future space-borne gravitational wave (GW) detector DECIGO in combination with the radio quasars as standard rulers. Similarly to the redshift drift in the electromagnetic domain, accelerating expansion of the Universe causes a characteristic phase correction to the gravitational waveform detectable by DECIGO. Hence, one would be able to extract the Hubble parameter $H(z)$. This could be used to recover distance-redshift relation supported by the data not relying on any specific cosmological model. Assuming the FLRW metric, and using intermediate luminosity radio quasars as standard rulers one achieves an interesting opportunity to directly assess $H_0$ and $\Omega_k$ parameters. To test this method we simulated a set of acceleration parameters achievable by future DECIGO. Based on the existing sample of 120 intermediate-luminosity radio-quasars calibrated as standard rulers, we simulated much bigger samples of such standard rulers possible to obtain with VLBI. In the case of $(N=100)$ of radio quasars, which is the size of currently available sample, the precision of cosmological parameters determined would be $\sigma_{H_0}=2.74$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ and $\sigma_{\Omega_k}=0.175$. In the optimistic scenario $(N = 1000)$ achievable by VLBI, the precision of $H_{0}$ would be improved to $1\%$, which is comparable to the result of $\sigma_{H_0} =0.54$ ${\mathrm{~km~s^{-1}~Mpc^{-1}}}$ from \emph{Planck} 2018 TT, TE, EE+lowE+lensing data, and the precision of $\Omega_k$ would be 0.050. Our results demonstrate that such combined analysis, possible in the future, could be helpful to solve the current cosmological issues concerning the Hubble tension and cosmic curvature tension.","Thu, 11 Apr 2024 01:21:20 UTC (232 KB)"
"214","Unveiling Behavioral Transparency of Protocols Communicated by IoT Networked Assets (Full Version)","Savindu Wannigama (1), Arunan Sivanathan (2), Ayyoob Hamza (2), Hassan Habibi Gharakheili (2) ((1) Department of Computer Engineering, University of Peradeniya, Sri Lanka. (2) School of EE&T, UNSW Sydney, Australia.)","Networking and Internet Architecture (cs.NI)","Behavioral transparency for Internet-of-Things (IoT) networked assets involves two distinct yet interconnected tasks: (a) characterizing device types by discerning the patterns exhibited in their network traffic, and (b) assessing vulnerabilities they introduce to the network. While identifying communication protocols, particularly at the application layer, plays a vital role in effective network management, current methods are, at best, ad-hoc. Accurate protocol identification and attribute extraction from packet payloads are crucial for distinguishing devices and discovering vulnerabilities. This paper makes three contributions: (1) We process a public dataset to construct specific packet traces pertinent to six standard protocols (TLS, HTTP, DNS, NTP, DHCP, and SSDP) of ten commercial IoT devices. We manually analyze TLS and HTTP flows, highlighting their characteristics, parameters, and adherence to best practices-we make our data publicly available; (2) We develop a common model to describe protocol signatures that help with the systematic analysis of protocols even when communicated through non-standard port numbers; and, (3) We evaluate the efficacy of our data models for the six protocols, which constitute approximately 97% of our dataset. Our data models, except for SSDP in 0.3% of Amazon Echo's flows, produce no false positives for protocol detection. We draw insights into how various IoT devices behave across those protocols by applying these models to our IoT traces.","Thu, 11 Apr 2024 00:47:14 UTC (88 KB)"
"215","Implementation of implicit filter for spatial spectra extraction","Kacper Nowak, Sergey Danilov, Vasco Müller, Caili Liu","Atmospheric and Oceanic Physics (physics.ao-ph)","Scale analysis based on coarse-graining has been proposed recently as an alternative to Fourier analysis. It is now broadly used to analyze energy spectra and energy transfers in eddy-resolving ocean simulations. However, for data from unstructured-mesh models it requires interpolation to a regular grid. We present a high-performance Python implementation of an alternative coarse-graining method which relies on implicit filters using discrete Laplacians. This method can work on arbitrary (structured or unstructured) meshes and is applicable to the direct output of unstructured-mesh ocean circulation atmosphere models. The computation is split into two phases: preparation and solving. The first one is specific only to the mesh. This allows for auxiliary arrays that are then computed to be reused, significantly reducing the computation time. The second part consists of sparse matrix algebra and solving linear system. Our implementation is accelerated by GPUs to achieve unmatched performance and scalability. This results in processing data based on meshes with more than 10M surface vertices in a matter of seconds. As an illustration, the method is applied to compute spatial spectra of ocean currents from high-resolution FESOM2 simulations.","Thu, 11 Apr 2024 00:19:15 UTC (6,261 KB)"
"216","Kinematic age of the $β$-Pictoris moving group","Jinhee Lee, Inseok Song","Solar and Stellar Astrophysics (astro-ph.SR)","Accurate age estimation of nearby young moving groups (NYMGs) is important as they serve as crucial testbeds in various fields of astrophysics, including formation and evolution of stars, planets, as well as loose stellar associations. The $\beta$-Pictoris moving group (BPMG), being one of the closest and youngest NYMGs, has been extensively investigated, and its estimated ages have a wide range from $\sim$10 to 25 Myr, depending on the age estimation methods and data used. Unlike other age dating methods, kinematic traceback analysis offers a model-independent age assessment hence the merit in comparing many seemingly discordant age estimates. In this study, we determine the kinematic ages of the BPMG using three methods: probabilistic volume calculation, mean pairwise distance calculation, and covariance matrix analysis. These methods yield consistent results, with estimated ages in the range of 14 to 20 Myr. Implementing corrections to radial velocities due to gravitational redshift and convectional blueshift increases the ages by $\sim2-4$ Myr. Conversely, considering data uncertainties decreases the estimated ages by 1 to 2 Myr. Taken together, our analysis determined the kinematic age of BPMG to be 16.3$^{+3.4}_{-2.1}$ Myr. This age is significantly younger than the commonly accepted age of the BPMG ($\sim$24 Myr) determined primarily from the lithium depletion boundary analysis. This younger kinematic age may point to the discrepancy between the luminosity evolution and lithium depletion models or the presence of unaccounted systematic error in the method. This result underscores the necessity for systematic reevaluations of age-dating methods for nearby, young moving groups.","Wed, 10 Apr 2024 23:49:38 UTC (3,315 KB)"
"217","ALMA-IMF XV: N$_2$H$^+$ kinematic analysis on the intermediate protocluster G353.41","R. H. Álvarez-Gutiérrez, A. M. Stutz, N. Sandoval-Garrido, F. Louvet, F. Motte, R. Galván-Madrid, N. Cunningham, P. Sanhueza, M. Bonfand, S. Bontemps, A. Gusdorf, T. Csengeri, S. D. Reyes, J. Salinas, T. Baug, L. Bronfman, G. Busquet, D. J. Díaz-González, M. Fernandez-Lopez, A. Guzmán, A. Koley, H.-L. Liu, F. A. Olguin, M. Valeille-Manet, F. Wyrowski","Astrophysics of Galaxies (astro-ph.GA)","The ALMA-IMF Large Program provides multi-tracer observations of 15 Galactic massive protoclusters at matched sensitivity and spatial resolution. We focus on the dense gas kinematics of the G353.41 protocluster traced by N$_2$H$^+$ (1$-$0), with a critical density of $2\times10^5$~cm$^{-3}$, and spatial resolution $\sim$0.02~pc. G353.41, at a distance of 2~kpc, is embedded in a larger scale ($\sim$8~pc) filament and has a mass of 2500~M$_{\odot}$ within $1.3\times1.3$~pc$^2$. We extract the N$_2$H$^+$ isolated line component and we decompose it by fitting up to 3 Gaussian velocity components. This allows us to identify velocity structures that are either muddled or impossible to identify in the traditional position-velocity diagram. We identify multiple velocity gradients (VGs) on large and small scales. We find good agreement between the N$_2$H$^+$ and the previously reported DCN core velocities, suggesting that cores are kinematically coupled to the dense gas in which they form. We measure 9 converging V-shaped VGs, located in filaments, that are sometimes associated with cores near their point of convergence. The average timescale associated with the V-shapes are $\sim$67~kyr, or about twice the free-fall time of cores in the same area ($\sim$~33~kyr) but substantially shorter than protostar lifetime estimates ($\sim$~0.5~Myr). We interpret these V-shapes as inflowing gas feeding the regions near cores and we derive their mass accretion rates. This feeding might lead to further filament collapse and formation of new cores. We suggest that the protocluster is collapsing on large scales, but the velocity signature of collapse is slow compared to pure free-fall. Thus these data are consistent with a comparatively slow global protocluster contraction under gravity, and faster core formation within, suggesting the formation of multiple generations of stars over the protocluster lifetime.","Wed, 10 Apr 2024 21:38:14 UTC (33,189 KB)"
"218","A Transformer-Based Model for the Prediction of Human Gaze Behavior on Videos","Suleyman Ozdel, Yao Rong, Berat Mert Albaba, Yen-Ling Kuo, Xi Wang","Computer Vision and Pattern Recognition (cs.CV)","Eye-tracking applications that utilize the human gaze in video understanding tasks have become increasingly important. To effectively automate the process of video analysis based on eye-tracking data, it is important to accurately replicate human gaze behavior. However, this task presents significant challenges due to the inherent complexity and ambiguity of human gaze patterns. In this work, we introduce a novel method for simulating human gaze behavior. Our approach uses a transformer-based reinforcement learning algorithm to train an agent that acts as a human observer, with the primary role of watching videos and simulating human gaze behavior. We employed an eye-tracking dataset gathered from videos generated by the VirtualHome simulator, with a primary focus on activity recognition. Our experimental results demonstrate the effectiveness of our gaze prediction method by highlighting its capability to replicate human gaze behavior and its applicability for downstream tasks where real human-gaze is used as input.","Wed, 10 Apr 2024 21:14:33 UTC (2,567 KB)"
"219","Recovering the gas properties of protoplanetary disks through parametric visibility modeling: MHO 6","Nicolas T. Kurtovic, Paola Pinilla","Earth and Planetary Astrophysics (astro-ph.EP)","The composition and distribution of the gas in a protoplanetary disk plays a key role in shaping the outcome of the planet formation process. Observationally, the recovery of information such as the emission height and brightness temperature from interferometric data is often limited by the imaging processes. To overcome the limitations of image-reconstruction when analyzing gas emission from interferometric observations, we have introduced a parametric model to fit the main observable properties of the gaseous disk component in the visibility plane. This approach is also known as parametric visibility modeling. We applied our parametric visibility modeling to the gas brightness distribution of the molecular line emission from 12CO J=3-2 and 13CO J=3-2 in the disk around MHO 6, a very-low-mass star in the Taurus star-forming Region. To improve the flux fidelity of our parametric models, we combined models with different pixel resolution before the computation of their visibilities, referred to as ``nesting images.'' When we apply our parametric visibility modeling to MHO 6, with independent fits to the emission from its CO isopotologues, the models return the same consistent results for the stellar mass, disk geometry, and central velocity. The surface height and brightness temperature distribution are also recovered. When compared to other disks, MHO 6 surface height is among the most elevated surfaces, consistent with the predictions for disks around very-low-mass stars. This work demonstrates the feasibility of running rapidly iterable parametric visibility models in moderate resolution and sensitivity interferometric observations. More importantly, this methodology opens the analysis of disk's gas morphology to observations where image-based techniques are unable to robustly operate, as in the case of the compact disk around MHO 6.","Wed, 10 Apr 2024 21:08:34 UTC (1,768 KB)"
"220","Determination of $K^0_S$ Fragmentation Functions including BESIII Measurements and using Neural Networks","Maryam Soleymaninia, Hadi Hashamipour, Maral Salajegheh, Hamzeh Khanpour, Hubert Spiesberger, Ulf-G. Meißner","High Energy Physics - Phenomenology (hep-ph)","In this study, we revisit the extraction of parton-to-$K^0_S$ hadron fragmentation functions, named FF24-$K^0_S$, focusing on both next-to-leading-order and next-to-next-to-leading-order accuracy in perturbative QCD. Our approach involves the analysis of single inclusive electron-positron annihilation (SIA) data. The two key improvements are, on the one hand, the incorporation of the latest experimental data from the BESIII experiment and, on the other hand, the adoption of Neural Networks in the fitting procedure. To address experimental uncertainties, the Monte Carlo method is employed. Our investigation also explores the impact of hadron mass corrections on the description of SIA data, spanning a broad kinematic regime with a particular emphasis on the range of small $z$ values. The theory prediction for $K^0_S$ production at both NLO and NNLO accuracy exhibits good agreement with experimental data within their respective uncertainties.","Wed, 10 Apr 2024 20:27:33 UTC (10,974 KB)"
"221","An analysis of parameter compression and full-modeling techniques with Velocileptors for DESI 2024 and beyond","M. Maus, S. Chen, M. White, J. Aguilar, S. Ahlen, A. Aviles, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, C. Howlett, M. Ishak, S. Juneau, A. Kremin, Y. Lai, M. Landriau, M. E. Levi, M. Manera, R. Miquel, E. Mueller, A. D. Myers, S. Nadathur, J. Nie, H. E. Noriega, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, S. Ramirez-Solano, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, B. A. Weaver, S. Yuan, P. Zarrouk, H. Zhang, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In anticipation of forthcoming data releases of current and future spectroscopic surveys, we present the validation tests and analysis of systematic effects within \texttt{velocileptors} modeling pipeline when fitting mock data from the \texttt{AbacusSummit} N-body simulations. We compare the constraints obtained from parameter compression methods to the direct fitting (Full-Modeling) approaches of modeling the galaxy power spectra, and show that the ShapeFit extension to the traditional template method is consistent with the Full-Modeling method within the standard $\Lambda$CDM parameter space. We show the dependence on scale cuts when fitting the different redshift bins using the ShapeFit and Full-Modeling methods. We test the ability to jointly fit data from multiple redshift bins as well as joint analysis of the pre-reconstruction power spectrum with the post-reconstruction BAO correlation function signal. We further demonstrate the behavior of the model when opening up the parameter space beyond $\Lambda$CDM and also when combining likelihoods with external datasets, namely the Planck CMB priors. Finally, we describe different parametrization options for the galaxy bias, counterterm, and stochastic parameters, and employ the halo model in order to physically motivate suitable priors that are necessary to ensure the stability of the perturbation theory.","Wed, 10 Apr 2024 19:21:33 UTC (18,470 KB)"
"222","Effective uniaxial dielectric function tensor and optical phonons in ($\bar{2}01$)-plane oriented $β$-Ga$_2$O$_3$ films with equally-distributed six-fold rotation domains","Alyssa Mock, Steffen Richter, Alexis Papamichail, Vallery Stanishev, Misagh Ghezellou, Jawad Ul-Hassan, Andreas Popp, Saud Bin Anooz, Daniella Gogova, Praneeth Ranga, Sriram Krishnamoorthy, Rafal Korlacki, Mathias Schubert, Vanya Darakchieva","Materials Science (cond-mat.mtrl-sci)","Monoclinic $\beta$-Ga$_2$O$_3$ films grown on $c$-plane sapphire have been shown to exhibit six $(\bar{2}01)$-plane oriented domains, which are equally-spaced-by-rotation around the surface normal and equally-sized-by-volume that render the film optical response effectively uniaxial. We derive and discuss an optical model suitable for ellipsometry data analysis of such films. We model mid- and far-infrared ellipsometry data from undoped and electrically insulating films with an effective uniaxial dielectric tensor based on projections of all phonon modes within the rotation domains parallel and perpendicular to the sample normal, i.e., to the reciprocal lattice vector $\mathbf{g}_{\bar{2}01}$. Two effective response functions are described by model, and found sufficient to calculate ellipsometry data that best-match measured ellipsometry data from a representative film. We propose to render either effective dielectric functions, or inverse effective dielectric functions, each separately for electric field directions parallel and perpendicular to $\mathbf{g}_{\bar{2}01}$, by sums of Lorentz oscillators, which permit to determine either sets of transverse optical phonon mode parameters, or sets of longitudinal optical phonon mode parameters, respectively. Transverse optical modes common to both dielectric functions can be traced back to single crystal modes with $B_{\mathrm{u}}$ character, while modes with $A_{\mathrm{u}}$ character only appear within the dielectric function for polarization perpendicular to the sample surface. The thereby obtained parameter sets reveal all phonon modes anticipated from averaging over the six-fold rotation domains of single crystal $\beta$-Ga$_2$O$_3$, but with slightly shifted transverse optical, and completely different longitudinal optical phonon modes.","Wed, 10 Apr 2024 18:49:42 UTC (1,647 KB)"
"223","Quantifying the Errors Introduced by Continuum Scattering Models on the Inferred Structural Properties of Proteins","Rohan S. Adhikari, Dilipkumar N. Asthagiri, Walter G. Chapman","Chemical Physics (physics.chem-ph)","Atomistic force fields that are tuned to describe folded proteins predict overly compact structures for intrinsically disordered proteins (IDPs). To correct this, improvements in force fields to better model IDPs are usually paired with scattering models for validation against experiments. For scattering calculations, protein configurations from all-atom simulations are used within the continuum-solvent model CRYSOL for comparison with experiments. To check this approach, we develop an equation to evaluate the radius of gyration (Rg) for any defined inner-hydration shell thickness given all-atom simulation data. Rg based on an explicit description of hydration waters compares well with the reference value of Rg obtained using Guinier analysis of the all-atom scattering model. However, these internally consistent estimates disagree with Rg from CRYSOL for the same definition of the inner-shell. CRYSOL can over-predict Rg by up to 2.5 Angstroms. We rationalize the reason for this behavior and highlight the consequences for force field design.","Wed, 10 Apr 2024 18:36:37 UTC (5,947 KB)"
"224","A comparison between Shapefit compression and Full-Modelling method with PyBird for DESI 2024 and beyond","Yan Lai, Cullan Howlett, Mark Maus, Héctor Gil-Marín, Hernan E. Noriega, Sadi Ramírez-Solano, Pauline Zarrouk, Jessica N. Aguilar, Steven Ahlen, Otávio Alves, Alejandro Aviles, David Brooks, Shi-Fan Chen, Todd Claybaugh, Tamara M. Davis, Kyle Dawson, Axel de la Macorra, Peter Doel, Jaime E. Forero-Romero, Enrique Gaztañaga, Satya Gontcho A Gontcho, Klaus Honscheid, Stephanie Juneau, Martin Landriau, Marc Manera, Ramon Miquel, Eva-Maria Mueller, Seshadri Nadathur, Gustavo Niz, Nathalie Palanque-Delabrouille, Will Percival, Claire Poppett, Mehdi Rezaie, Graziano Rossi, Eusebio Sanchez, Michael Schubnell, David Sprayberry, Gregory Tarlé, Mariana Vargas-Magaña, Licia Verde, Sihan Yuan, Rongpu Zhou, Hu Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","DESI aims to provide one of the tightest constraints on cosmological parameters by analyzing the clustering of more than thirty million galaxies. However, obtaining such constraints requires special care in validating the analysis methods, and efforts to reduce the computational time required through techniques such as data compression and emulation. In this work, we perform a precision validation of the PyBird power spectrum modelling code with both a traditional, but emulated, Full-Modelling approach and the model-independent Shapefit compression approach. Using cubic simulations, which accurately reproduce the clustering and precision of the DESI survey, we find that the cosmological constraints from Shapefit and Full-Modelling are consistent with each other at the $\sim0.3\sigma$ level. Both Shapefit and Full-Modelling are also consistent with the true $\Lambda$CDM simulation cosmology, even when including the hexadecapole, down to a scale $k_{\mathrm{max}} = 0.20 h \mathrm{Mpc}^{-1}$. For extended models such as the $w$CDM and the $o$CDM models, we find including the hexadecapole can significantly improve the constraints and reduce the systematic errors with the same $k_{\mathrm{max}}$. Furthermore, we also show that the constraints on cosmological parameters with the correlation function evaluated from PyBird down to $s_{\mathrm{min}} = 30 h^{-1} \mathrm{Mpc}$ are unbiased, and consistent with the constraints from the power spectrum.","Wed, 10 Apr 2024 18:26:16 UTC (18,726 KB)"
"225","Validating the Galaxy and Quasar Catalog-Level Blinding Scheme for the DESI 2024 analysis","U. Andrade, J. Mena-Fernández, H. Awan, A. J. Ross, S. Brieden, J. Pan, A. de Mattia, J. Aguilar, S. Ahlen, O. Alves, D. Brooks, E. Buckley-Geer, E. Chaussidon, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, J. Guy, C. Hahn, M. M. S Hanif, K. Honscheid, C. Howlett, D. Huterer, S. Juneau, A. Kremin, M. Landriau, L. Le Guillou, M. E. Levi, M. Manera, P. Martini, A. Meisner, R. Miquel, J. Moustakas, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. A. Newman, J. Nie, G. Niz, N. Palanque-Delabrouille, W. J. Percival, M. Pinon, C. Poppett, F. Prada, M. Rashkovetskyi, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, M. Vargas-Magaña, L. Verde, B. A. Weaver","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the era of precision cosmology, ensuring the integrity of data analysis through blinding techniques is paramount -- a challenge particularly relevant for the Dark Energy Spectroscopic Instrument (DESI). DESI represents a monumental effort to map the cosmic web, with the goal to measure the redshifts of tens of millions of galaxies and quasars. Given the data volume and the impact of the findings, the potential for confirmation bias poses a significant challenge. To address this, we implement and validate a comprehensive blind analysis strategy for DESI Data Release 1 (DR1), tailored to the specific observables DESI is most sensitive to: Baryonic Acoustic Oscillations (BAO), Redshift-Space Distortion (RSD) and primordial non-Gaussianities (PNG). We carry out the blinding at the catalog level, implementing shifts in the redshifts of the observed galaxies to blind for BAO and RSD signals and weights to blind for PNG through a scale-dependent bias. We validate the blinding technique on mocks, as well as on data by applying a second blinding layer to perform a battery of sanity checks. We find that the blinding strategy alters the data vector in a controlled way such that the BAO and RSD analysis choices do not need any modification before and after unblinding. The successful validation of the blinding strategy paves the way for the unblinded DESI DR1 analysis, alongside future blind analyses with DESI and other surveys.","Wed, 10 Apr 2024 18:24:08 UTC (8,180 KB)"
"226","A comparison of effective field theory models of redshift space galaxy power spectra for DESI 2024 and future surveys","M. Maus, Y. Lai, H. E. Noriega, S. Ramirez-Solano, A. Aviles, S. Chen, S. Fromenteau, H. Gil-Marín, C. Howlett, M. Vargas-Magaña, M. White, P. Zarrouk, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, E. Burtin, T. Claybaugh, S. Cole, K. Dawson, M. Icaza-Lizaola, A. de la Macorra, A. de Mattia, P. Doel, S. Ferraro, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, C. Hahn, K. Honscheid, M. Ishak, A. Kremin, M. Landriau, L. Le Guillou, M. Manera, R. Miquel, E. Mueller, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, F. Prada, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, S. Yuan, R. Zhao, R. Zhou, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In preparation for the next generation of galaxy redshift surveys, and in particular the year-one data release from the Dark Energy Spectroscopic Instrument (DESI), we investigate the consistency of a variety of effective field theory models that describe the galaxy-galaxy power spectra in redshift space into the quasi-linear regime using 1-loop perturbation theory. These models are employed in the pipelines \texttt{velocileptors}, \texttt{PyBird}, and \texttt{Folps$\nu$}. While these models have been validated independently, a detailed comparison with consistent choices has not been attempted. After briefly discussing the theoretical differences between the models we describe how to provide a more apples-to-apples comparison between them. We present the results of fitting mock spectra from the \texttt{AbacusSummit} suite of N-body simulations provided in three redshift bins to mimic the types of dark time tracers targeted by the DESI survey. We show that the theories behave similarly and give consistent constraints in both the forward-modeling and ShapeFit compressed fitting approaches. We additionally generate (noiseless) synthetic data from each pipeline to be fit by the others, varying the scale cuts in order to show that the models agree within the range of scales for which we expect 1-loop perturbation theory to be applicable. This work lays the foundation of Full-Shape analysis with DESI Y1 galaxy samples where in the tests we performed, we found no systematic error associated with the modeling of the galaxy redshift space power spectrum for this volume.","Wed, 10 Apr 2024 18:03:38 UTC (8,310 KB)"
"227","Comparing Compressed and Full-modeling Analyses with FOLPS: Implications for DESI 2024 and beyond","H. E. Noriega, A. Aviles, H. Gil-Marín, S. Ramirez-Solano, S. Fromenteau, M. Vargas-Magaña, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, J. L. Cervantes-Cota, S. Chen, T. Claybaugh, S. Cole, K. Dawson, A. de la Macorra, A. de Mattia, P. Doel, N. Findlay, J. E. Forero-Romero, E. Gaztañaga, S. Gontcho A Gontcho, K. Honscheid, J. Hou, C. Howlett, M. Ishak, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, G. Morales-Navarrete, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, G. Niz, N. Palanque-Delabrouille, W. J. Percival, C. Poppett, M. Rezaie, A. Rocher, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, D. Sprayberry, G. Tarlé, L. Verde, S. Yuan, P. Zarrouk, H. Zou","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","The Dark Energy Spectroscopic Instrument (DESI) will provide unprecedented information about the large-scale structure of our Universe. In this work, we study the robustness of the theoretical modelling of the power spectrum of FOLPS, a novel effective field theory-based package for evaluating the redshift space power spectrum in the presence of massive neutrinos. We perform this validation by fitting the AbacusSummit high-accuracy $N$-body simulations for Luminous Red Galaxies, Emission Line Galaxies and Quasar tracers, calibrated to describe DESI observations. We quantify the potential systematic error budget of FOLPS, finding that the modelling errors are fully sub-dominant for the DESI statistical precision within the studied range of scales. Additionally, we study two complementary approaches to fit and analyse the power spectrum data, one based on direct Full-Modelling fits and the other on the ShapeFit compression variables, both resulting in very good agreement in precision and accuracy. In each of these approaches, we study a set of potential systematic errors induced by several assumptions, such as the choice of template cosmology, the effect of prior choice in the nuisance parameters of the model, or the range of scales used in the analysis. Furthermore, we show how opening up the parameter space beyond the vanilla $\Lambda$CDM model affects the DESI observables. These studies include the addition of massive neutrinos, spatial curvature, and dark energy equation of state. We also examine how relaxing the usual Cosmic Microwave Background and Big Bang Nucleosynthesis priors on the primordial spectral index and the baryonic matter abundance, respectively, impacts the inference on the rest of the parameters of interest. This paper pathways towards performing a robust and reliable analysis of the shape of the power spectrum of DESI galaxy and quasar clustering using FOLPS.","Wed, 10 Apr 2024 18:00:54 UTC (12,699 KB)"
"228","Full Modeling and Parameter Compression Methods in configuration space for DESI 2024 and beyond","S. Ramirez-Solano, M. Icaza-Lizaola, H. E. Noriega, M. Vargas-Magaña, S. Fromenteau, A. Aviles, F. Rodriguez-Martinez, J. Aguilar, S. Ahlen, O. Alves, S. Brieden, D. Brooks, T. Claybaugh, S. Cole, A. de la Macorra, Arjun Dey, B. Dey, P. Doel, K. Fanning, J. E. Forero-Romero, E. Gaztañaga, H. Gil-Marín, S. Gontcho A Gontcho, K. Honscheid, C. Howlett, S. Juneau, Y. Lai, M. Landriau, M. Manera, M. Maus, R. Miquel, E. Mueller, A. Muñoz-Gutiérrez, A. D. Myers, S. Nadathur, J. Nie, W. J. Percival, C. Poppett, M. Rezaie, G. Rossi, E. Sanchez, D. Schlegel, M. Schubnell, H. Seo, D. Sprayberry, G. Tarlé, L. Verde, B. A. Weaver, R. H. Wechsler, S. Yuan, P. Zarrouk, H. Zou (DESI Collaboration)","Cosmology and Nongalactic Astrophysics (astro-ph.CO)","In the contemporary era of high-precision spectroscopic surveys, led by projects like DESI, there is an increasing demand for optimizing the extraction of cosmological information from clustering data. This work conducts a thorough comparison of various methodologies for modeling the full shape of the two-point statistics in configuration space. We investigate the performance of both direct fits (Full-Modeling) and the parameter compression approaches (ShapeFit and Standard). We utilize the ABACUS-SUMMIT simulations, tailored to exceed DESI's precision requirements. Particularly, we fit the two-point statistics of three distinct tracers (LRG, ELG, and QSO), by employing a Gaussian Streaming Model in tandem with Convolution Lagrangian Perturbation Theory and Effective Field Theory. We explore methodological setup variations, including the range of scales, the set of galaxy bias parameters, the inclusion of the hexadecapole, as well as model extensions encompassing varying $n_s$ and allowing for $w_0w_a$CDM dark energy model. Throughout these varied explorations, while precision levels fluctuate and certain configurations exhibit tighter parameter constraints, our pipeline consistently recovers the parameter values of the mocks within $1\sigma$ in all cases for a 1-year DESI volume. Additionally, we compare the performance of configuration space analysis with its Fourier space counterpart using three models: PyBird, FOLPS and velocileptors, presented in companion papers. We find good agreement with the results from all these models.","Wed, 10 Apr 2024 18:00:53 UTC (8,730 KB)"
"229","Complete Optimal Non-Resonant Anomaly Detection","Gregor Kasieczka, John Andrew Raine, David Shih, Aman Upadhyay","High Energy Physics - Phenomenology (hep-ph)","We propose the first-ever complete, model-agnostic search strategy based on the optimal anomaly score, for new physics on the tails of distributions. Signal sensitivity is achieved via a classifier trained on auxiliary features in a weakly-supervised fashion, and backgrounds are predicted using the ABCD method in the classifier output and the primary tail feature. The independence between the classifier output and the tail feature required for ABCD is achieved by first training a conditional normalizing flow that yields a decorrelated version of the auxiliary features; the classifier is then trained on these features. Both the signal sensitivity and background prediction require a sample of events accurately approximating the SM background; we assume this can be furnished by closely related control processes in the data or by accurate simulations, as is the case in countless conventional analyses. The viability of our approach is demonstrated for signatures consisting of (mono)jets and missing transverse energy, where the main SM background is $Z(\nu \nu) +\text{jets}$, and the data-driven control process is $\gamma+\text{jets}$.","Wed, 10 Apr 2024 18:00:01 UTC (859 KB)"
